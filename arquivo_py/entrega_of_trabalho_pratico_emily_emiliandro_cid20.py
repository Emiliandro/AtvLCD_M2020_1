# -*- coding: utf-8 -*-
"""Entrega of Trabalho_Pratico_Emily_Emiliandro_cid20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EIv0p2M9gol_P1DWvtcLyDpq022a79-8

# Introdução ao Projeto

  Devido à rápida propagação da doença COVID-19 no mundo, as plataformas de mídias sociais como Twitter, Facebook e Instagram tornaram-se locais onde ocorre uma intensa e contínua troca de informações entre diversas partes e grupos sociais da sociedade. O seguinte trabalho se propõem analisar e tratar dados obtidos na rede social twitter para melhor compreensão da opinião pública nos meses iniciais.

## Equipe do projeto

O projeto foi feito pela dupla de alunos Emily Bezerra Sales (ebs.cid20@uea.edu.br) e Emiliandro Carlos de Moraes Firmino (ecdmf.cid20@uea.edu.br) para o módulo de Programação ministrada pelo Prof. Dr. Tiago Eugenio de Melo (tmelo@uea.edu.br).

## Dependências

Para a edição do matérial fornecido, foi utilizado a linguagem python junto de suas bibliotecas de manipulação de Array, Data(time) e String para compreensão dos textos, dos autores das mensagens e da data de publicação.
"""

import string
import array
import datetime
import operator

"""### Google Drive

A integração com o google drive foi feito para permitir o acesso ao arquivo principal do projeto, pois devido a seu tamanho houve a necessidade do uso de um sistema de host que permitisse arquivos de grande porte - 350mb no caso.
"""

from google.colab import drive

"""### Numpy

A biblioteca Numpy foi utilizado para facilitar na programação presente no projeto.
"""

import numpy as np

"""### Pandas

A biblioteca Pandas foi adicionado para permitir a interpretação e visualização de arquivos .csv em formato de tabela.
"""

import pandas as pd

"""### CSV

A biblioteca CSV foi adiciona para permitir a exportação de arquivo .csv para uso no projeto.
"""

import csv

"""### Matplot

A biblioteca matplot foi utilizada para permitir a criação de gráficos para o projeto.
"""

import matplotlib.pyplot as plt

"""### Folium

A biblioteca folium foi utilizada para a exibição de mapas com base na geolocalização
"""

import folium
from folium.plugins import HeatMap

"""# Preparação para a Leitura de dados"""

dados_completos = {} # csv fornecido pelo professor

"""## Integração do Google Drive"""

#@title Carregar Dados Completos (Google Drive) { form-width: "45%" }

montar_drive = True #@param {type:"boolean"}
carregar_completos = True #@param {type:"boolean"}

if (montar_drive): 
  drive.mount('/content/drive')

if(carregar_completos):
  dados_completos = pd.read_csv("/content/drive/My Drive/dados-curso-completo.csv")

"""## Primeiro Uso do Panda"""

pd.DataFrame(dados_completos)

"""## Dados Nulos

O processo de verificação de duplicatas se iniciou através da observação de arquivos nulos para observar qual o melhor parametro para ser usado. Através do método abaixo foi decidido utilizar os três parametros de texto, data e apelido para montar o arquivo .csv filtrado.
"""

dados_completos.isnull().sum() #contando (identificando) dados faltantes

"""### Limpando Dados Nulos

Em equipe foi definido a remoção dos dados nulos para a filtragem de dados, ademais foi criado um método para ser chamado sempre que possível no projeto onde é removido os dados nulos e retornado uma cópia limpa.
"""

def Limpando_nulos(arquivo):
  arquivo.dropna (inplace=True) #limpando dados nulos de todo o banco
  return arquivo.copy()

dados_completos_sem_nulo = Limpando_nulos(dados_completos) #csv para filtragem de duplicata

dados_completos_sem_nulo.isnull().sum()

"""## Dados em Português"""

regioes_e_idiomas = dados_completos_sem_nulo.filter(items=['pais','idioma'])

"""### Idiomas

Através de uma observação de dados totais presentes na coluna de idioma foi percebido uma gama de idiomas e necessário tomar uma decisão de como trabalhar com eles, assim, foi decidio trabalhar apenas com os dados em português e focar nessas pessoas.
"""

regioes_e_idiomas.groupby('idioma').count().sort_values(by='idioma',ascending=True) #tentando filtrar e ordenar por quantidade

"""### Regiões

Sabendo a quantia de falantes do idioma português, surgiu a duvida de onde estão localizadas essas pessoas. Quais são os paises presentes nos dados fornecidos, abaixo a quantia.
"""

regioes_e_idiomas.groupby('pais').count().sort_values(by='pais',ascending=True) #tentando filtrar e ordenar por quantidade

"""### Script de Verificação de Idioma"""

def Check_Em_Portugues(idioma):
  if(idioma == 'pt'):
    return True
  else:
    return False

"""### Total de Dados em Português"""

total_presente = len(dados_completos_sem_nulo)
contador_em_pt = 0
contador_no_br = 0
contador_fora_br = 0
for i in range(total_presente):

  # pegando valores de idioma e sigla do país em texto
  valor_i_idioma = dados_completos_sem_nulo.iloc[i]['idioma'].lower()
  valor_i_local = dados_completos_sem_nulo.iloc[i]['sigla'].lower()
  
  if (i == 0):
    contador_em_pt = 0 # limpando valor
    contador_no_br = 0 # limpando valor
    contador_fora_br = 0 #limpando valor

  if (Check_Em_Portugues(valor_i_idioma)):
    # somando dados em português
    contador_em_pt +=1
    
    if (valor_i_local == "br"):
      # somando dados no Brasil
      contador_no_br +=1
    else:
      # somando dados fora do Brasil
      contador_fora_br +=1

print("\n Foram examinados",total_presente,'linhas',
      "\n Desses",contador_em_pt,"estão em português",
      "\n Desses",contador_no_br,"estão no Brasil e ",
      "\n Desses",contador_fora_br,"estão fora do Brasil")

labels = 'no Brasil', 'fora do Brasil'
sections = [contador_no_br, contador_fora_br]
colors = ['g', 'r']

plt.pie(sections, labels=labels, colors=colors,
        startangle=180,
        explode = (0, 0.4),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('Total de Pessoas (Dados Não Nulos) com Texto em Português (Com Duplicatas) \n')
plt.show()

"""## Remoção de Duplicatas

### Organização por ordem alfabética de apelido
"""

# total de dados para se trabalhar
num_dataset_filtrado = len(dados_completos_sem_nulo)

# arrays para montar dataset
dic_texto = [] # array para conter a coluna texto
dic_lugar = [] # array para conter a coluna lugar
dic_pais = [] # array para conter a coluna pais
dic_data = [] # array para conter a coluna data
dic_apelido = [] # array para coluna apelido
dic_latitude = [] # array para conter a coluna latitude
dic_longitude = [] # array para conter a coluna longitude
dic_idioma = [] # array para conter a coluna idioma
dic_sigla = [] # array para conter a coluna sigla
dic_resumo = [] # array para conter a nova coluna resumo


for i in range(num_dataset_filtrado):
  # formatando o texto obtido
  texto_filtrado = dados_completos_sem_nulo.iloc[i]['texto'].lower()
  dic_texto.append(texto_filtrado)
  # adicionando os demais dados
  # Os dados foram separados em arrays para facilitar a ordenação
  dic_lugar.append(dados_completos_sem_nulo.iloc[i]['lugar'].lower())
  dic_pais.append(dados_completos_sem_nulo.iloc[i]['pais'].lower())
  dic_latitude.append(dados_completos_sem_nulo.iloc[i]['latitude'])
  dic_longitude.append(dados_completos_sem_nulo.iloc[i]['longitude'])
  dic_sigla.append(dados_completos_sem_nulo.iloc[i]['sigla'])
  dic_idioma.append(dados_completos_sem_nulo.iloc[i]['idioma'].lower())
  dic_data.append(dados_completos_sem_nulo.iloc[i]['data'])
  # nova coluna resumo
  texto_resumido = texto_filtrado.translate({ord(c): None for c in string.whitespace})
  dic_resumo.append(texto_resumido[0:20])
  # um marcador foi adicionado ao apelido para filtar itens no array
  dic_apelido.append(dados_completos_sem_nulo.iloc[i]['apelido'].lower() + "+" + str(i))

# um sort foi feito com base no apelido para a reordenação
dic_apelido.sort()

# montagem do dicionário organizado pela ordem de apelido
dicionario_apelido_ordem_alfabetica = []
contador_de_erros = 0 # informacional 

for i in range(len(dic_apelido)):
  posicao = dic_apelido[i].find("+")  #filtro o comeco da posicao do marcador
  posicao_maxima = len(dic_apelido[i]) # filtro o fim da posicao do marcador
  apelido_formatado = dic_apelido[i][0:posicao] # filtro o marcador
  try:
    #tendo o marcador em maos transformo de string para inteiro
    posicao_formatado = int(dic_apelido[i][posicao+1:posicao_maxima])
    try:
      # utilizo o inteiro para achar o valor e montar o dicionario
      data_list = {'id': str(i), # valor posicao para uso futuro (facilitador)
                   'apelido':apelido_formatado,
                   'texto':dic_texto[posicao_formatado],
                   'data':dic_data[posicao_formatado],
                   'idioma':dic_idioma[posicao_formatado],
                   'pais':dic_pais[posicao_formatado],
                   'latitude':dic_latitude[posicao_formatado],
                   'longitude':dic_longitude[posicao_formatado],
                   'sigla':dic_sigla[posicao_formatado],
                   'lugar':dic_lugar[posicao_formatado],
                   'resumo':dic_resumo[posicao_formatado]}
      dicionario_apelido_ordem_alfabetica.append(data_list)
    except IndexError:
      print("index error em ",posicao_formatado,posicao)
  except (ValueError):
    contador_de_erros +=1
    print(str(contador_de_erros),dic_texto[i])

pd.DataFrame(dicionario_apelido_ordem_alfabetica).tail()

"""Através da ordem alfabetica se torna vísivel as duplicatas e em seguida, ao observar suas datas, foi possível constantar uma serie de regras para definir o que são as duplicatas .

### Regras que define um texto repetido


1.   Mesmo apelido que postou o mesmo texto: as repostagens.
2.   Diferentes apelidos que postou o mesmo texto: as repostagens de sites de notícias ou de links alheios.

### Download do arquivo por ordem alfabética de apelido
"""

#@title Baixar dados não nulos ordenado por apelido em .csv  { form-width: "45%" }


gerar_download = True #@param {type:"boolean"}
dic_para_download = {}

if(gerar_download):
  for di in dicionario_apelido_ordem_alfabetica:
    dic_para_download[di['id']]={}
    for k in di.keys():
      if k =='id': continue
      dic_para_download[di['id']][k]=di[k]
      
  with open('DadosNaoNuloFrmtApelido.csv', 'w') as csv_file:  
    writer = csv.writer(csv_file)
    writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                     'idioma','pais','latitude','longitude','sigla','lugar','resumo'])
    for key, value in dic_para_download.items():
      writer.writerow([key, value['apelido'], value['data'], 
                       value['texto'], value['idioma'],
                       value['pais'], value['latitude'],
                       value['longitude'], value['sigla'],
                       value['lugar'],value['resumo'] ])

"""## Os Textos Repetidos"""

#@title Usar dados do drive
usar_drive = True #@param {type:"boolean"}
url_para_usar = "https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosNaoNuloFrmtApelido%20(1).csv"
if (usar_drive):
  url_para_usar = "/content/DadosNaoNuloFrmtApelido.csv"

csv_para_usar = pd.read_csv(url_para_usar)
textos_repetidos = csv_para_usar.filter(items=['resumo','apelido'])
textos_repetidos.groupby('resumo').count().sort_values(by='resumo',ascending=True) #tentando filtrar e ordenar por quantidade

csv_para_usar.drop_duplicates(subset="resumo",keep="last")

"""## Ordem de publicação"""

num_dataset_filtrado = len(csv_para_usar)

# arrays para montar dataset
dic_texto = [] # array para conter a coluna texto
dic_lugar = [] # array para conter a coluna lugar
dic_pais = [] # array para conter a coluna pais
dic_data = [] # array para conter a coluna data
dic_apelido = [] # array para coluna apelido
dic_latitude = [] # array para conter a coluna latitude
dic_longitude = [] # array para conter a coluna longitude
dic_idioma = [] # array para conter a coluna idioma
dic_sigla = [] # array para conter a coluna sigla
dic_resumo = [] # array para conter a nova coluna resumo


for i in range(num_dataset_filtrado):
  # formatando o texto obtido
  texto_filtrado = csv_para_usar.iloc[i]['texto'].lower()
  dic_texto.append(texto_filtrado)
  # adicionando os demais dados
  # Os dados foram separados em arrays para facilitar a ordenação
  dic_lugar.append(csv_para_usar.iloc[i]['lugar'].lower())
  dic_pais.append(csv_para_usar.iloc[i]['pais'].lower())
  dic_latitude.append(csv_para_usar.iloc[i]['latitude'])
  dic_longitude.append(csv_para_usar.iloc[i]['longitude'])
  dic_sigla.append(csv_para_usar.iloc[i]['sigla'])
  dic_idioma.append(csv_para_usar.iloc[i]['idioma'].lower())
  dic_apelido.append(csv_para_usar.iloc[i]['apelido'])
  texto_resumido = texto_filtrado.translate({ord(c): None for c in string.whitespace})
  dic_resumo.append(texto_resumido[0:20])
  # um marcador foi adicionado ao apelido para filtar itens no array
  dic_data.append(csv_para_usar.iloc[i]['data'].lower() + "+" + str(i))

# um sort foi feito com base no apelido para a reordenação
dic_data.sort()

# montagem do dicionário organizado pela ordem de apelido
dicionario_ordem_data = []
contador_de_erros = 0 # informacional 

for i in range(len(dic_data)):
  posicao = dic_data[i].find("+")  #filtro o comeco da posicao do marcador
  posicao_maxima = len(dic_data[i]) # filtro o fim da posicao do marcador
  data_formatado = dic_data[i][0:posicao] # filtro o marcador
  try:
    #tendo o marcador em maos transformo de string para inteiro
    posicao_formatado = int(dic_data[i][posicao+1:posicao_maxima])
    try:
      # utilizo o inteiro para achar o valor e montar o dicionario
      data_list = {'id': str(i), # valor posicao para uso futuro (facilitador)
                   'apelido':dic_apelido[posicao_formatado],
                   'texto':dic_texto[posicao_formatado],
                   'data':data_formatado,
                   'idioma':dic_idioma[posicao_formatado],
                   'pais':dic_pais[posicao_formatado],
                   'latitude':dic_latitude[posicao_formatado],
                   'longitude':dic_longitude[posicao_formatado],
                   'sigla':dic_sigla[posicao_formatado],
                   'lugar':dic_lugar[posicao_formatado],
                   'resumo':dic_resumo[posicao_formatado]}
      dicionario_ordem_data.append(data_list)
    except IndexError:
      print("index error em ",posicao_formatado,posicao)
  except (ValueError):
    contador_de_erros +=1
    print(str(contador_de_erros),dic_texto[i])

pd.DataFrame(dicionario_ordem_data).tail()

#@title Baixar dados não nulos ordenado por data em .csv  { form-width: "45%" }


gerar_download = True #@param {type:"boolean"}
dic_para_download = {}

if(gerar_download):
  for di in dicionario_ordem_data:
    dic_para_download[di['id']]={}
    for k in di.keys():
      if k =='id': continue
      dic_para_download[di['id']][k]=di[k]
      
  with open('DadosOrdemData.csv', 'w') as csv_file:  
    writer = csv.writer(csv_file)
    writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                     'idioma','pais','latitude','longitude','sigla','lugar','resumo'])
    for key, value in dic_para_download.items():
      writer.writerow([key, value['apelido'], value['data'], 
                       value['texto'], value['idioma'],
                       value['pais'], value['latitude'],
                       value['longitude'], value['sigla'],
                       value['lugar'],value['resumo'] ])

#@title Exibir Resultado Filtrado  { form-width: "45%" }
usar_drive = True #@param {type:"boolean"}

url_para_usar = "https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv"

if (usar_drive):
  url_para_usar = "/content/DadosOrdemData.csv"

exibir_resultado = True #@param {type:"boolean"}
if (exibir_resultado):
  dados_ordem_data = pd.read_csv(url_para_usar)
  dados_completos = pd.read_csv("/content/drive/My Drive/dados-curso-completo.csv")

  num_total = len(dados_completos)
  num_or_data = len(dados_ordem_data)
  num_cortado = num_total - num_or_data
  print("De",num_total,"sobraram",num_or_data,"linhas \n")

  labels = 'Faltando Dados ou Duplicado', 'Inteiro'
  sections = [num_cortado, num_or_data]
  colors = ['y', 'r']

  plt.pie(sections, labels=labels, colors=colors,
          startangle=180,
          explode = (0, 1),
          autopct = '%1.2f%%')

  plt.axis('equal') # Try commenting this out.
  plt.title('Total de Dados Preparados \n')
  plt.show()

"""## Script de Filtro de Dados

### Método da Filtragem
"""

def filtrar_e_exportar(csv_usado,formatar,palavra,exportar,nome):      
  obj_para_download = []
  dic_para_download = {}

  # ----------------------
  # FILTRANDO DADOS DOWNLOAD
  # ----------------------
  num_dataset_filtrado = len(csv_usado)
  ids_filtrados = []
  num_filtrados = 0
  for i in range(num_dataset_filtrado):
    filtro1 = csv_usado.iloc[i]['texto'].lower()
    filtro2 = filtro1.replace("?"," ? ") # deixar a interrogacao mais visivel
    filtro3 = filtro2.replace("#"," ") # remove hashtags
    filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
    filtro5 = filtro4.replace("í","i") # remove quebra de linha
    palavras = filtro5.split(' ') # forma array
    adicionado = False

    if (i>=0):
    # if (formatar):
    #   edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})
    #   filtro1 = edit_filtro1
      for d in range(len(palavras)):
        if (d==0):
          adicionado = False #debug
        if (palavras[d]==palavra):
          # print('achou valor')
          num_filtrados +=1
          if (adicionado == False):
            adicionado = True
            ids_filtrados.append(csv_usado.iloc[i]['posicao'])
      # try:
      #   valor = filtro1.find(palavra)
      #   if (valor!=-1):
      #     num_filtrados +=1
      #     ids_filtrados.append(i)
      # except:
      #   print("erro")
      if (i == num_dataset_filtrado-1):
        print("\n Filtragem da palavra",palavra,"terminou",
              "\n Foram filtrados um total de", len(ids_filtrados), "itens",
              "\n sendo eles",ids_filtrados)
      
  # ----------------------
  # FORMATAR DOWNLOAD
  # ----------------------

  for i in range(len(ids_filtrados)):
      posicao_formatado = ids_filtrados[i]
      data_list = {'id': str(i),
                  'apelido':csv_usado.iloc[posicao_formatado]['apelido'],
                  'texto':csv_usado.iloc[posicao_formatado]['texto'],
                  'data':csv_usado.iloc[posicao_formatado]['data'],
                  'idioma':csv_usado.iloc[posicao_formatado]['idioma'],
                  'pais':csv_usado.iloc[posicao_formatado]['pais'],
                  'latitude':csv_usado.iloc[posicao_formatado]['latitude'],
                  'longitude':csv_usado.iloc[posicao_formatado]['longitude'],
                  'sigla':csv_usado.iloc[posicao_formatado]['sigla'],
                  'lugar':csv_usado.iloc[posicao_formatado]['lugar']}
      obj_para_download.append(data_list)

  # ----------------------
  # EXPORTANDO CSV
  # ----------------------

  gerarDownload = exportar
  nome_csv = "datasheet_" + str(palavra)
  if(nome != ""):
    nome_csv = nome_do_arquivo

  if(gerarDownload):
    for di in obj_para_download:
      dic_para_download[di['id']]={}
      for k in di.keys():
        if k =='id': continue
        dic_para_download[di['id']][k]=di[k]
        
    with open( nome_csv +'.csv', 'w') as csv_file:  
      writer = csv.writer(csv_file)
      writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                      'idioma','pais','latitude','longitude','sigla',
                      'lugar','resumo'])
      for key, value in dic_para_download.items():
        writer.writerow([key, value['apelido'], value['data'], 
                        value['texto'], value['idioma'],
                        value['pais'], value['latitude'],
                        value['longitude'], value['sigla'],
                        value['lugar'] ])

"""### Uso do Método de Filtragem"""

#@title Opções de Filtragem { form-width: "70%" }
qual_palavra = "italia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar: pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

"""# Interpretação de Dados

## Primeira Conclusão da Interpretação
"""

dados_completos = pd.read_csv("/content/drive/My Drive/dados-curso-completo.csv")
numero_lugares_unicos_completo = len(dados_completos.filter(items=['lugar','apelido']).groupby('lugar'))
numero_contas_unicas_completo = len(dados_completos.filter(items=['lugar','apelido']).groupby('apelido'))
numero_msg_unicas_completo = len(dados_completos.filter(items=['texto','apelido']).groupby('texto'))
numero_paises_unicos_completo = len(dados_completos.filter(items=['pais','apelido']).groupby('pais'))
numero_locacoes_unicos_completo = len(dados_completos.filter(items=['latitude','apelido']).groupby('apelido'))
numero_idioma_unicos_completo = len(dados_completos.filter(items=['idioma','apelido']).groupby('idioma'))

print("\n De",numero_contas_unicas_completo,"contas totais",
      "\n ha",numero_msg_unicas_completo,"mensagens não repetidas",
      "\n de",numero_idioma_unicos_completo,"idiomas diferentes",
      "\n de",numero_paises_unicos_completo,"paises",
      "\n e",numero_lugares_unicos_completo,"cidades",
      "\n e de",numero_locacoes_unicos_completo,"latitudes diferentes")

"""Através do uso da biblioteca Pandas foi possível perceber que do dataset fornecido pelo professor existem 774516 contas totais com um total de 1653599 mensagens não repetidas de 59 idiomas diferentes vindas de 121 paises, totalizando 4500 cidades e 774516 latitudes diferentes"""

csv_para_usar = pd.read_csv("/content/DadosOrdemData.csv")
numero_lugares_unicos = len(csv_para_usar.filter(items=['lugar','apelido']).groupby('lugar'))
numero_contas_unicas = len(csv_para_usar.filter(items=['lugar','apelido']).groupby('apelido'))
numero_msg_unicas = len(csv_para_usar.filter(items=['resumo','apelido']).groupby('resumo'))
numero_paises_unicos = len(csv_para_usar.filter(items=['pais','apelido']).groupby('pais'))
numero_locacoes_unicos = len(csv_para_usar.filter(items=['latitude','apelido']).groupby('apelido'))
numero_idioma_unicos = len(csv_para_usar.filter(items=['idioma','apelido']).groupby('idioma'))

print("\n De",numero_contas_unicas,"contas totais",
      "\n ha",numero_msg_unicas,"mensagens não repetidas",
      "\n de",numero_idioma_unicos,"idiomas diferentes",
      "\n de",numero_paises_unicos,"paises",
      "\n e",numero_lugares_unicos,"cidades",
      "\n e de",numero_locacoes_unicos,"latitudes diferentes")

"""A filtragem de dados não foi perfeita, pois com a filtragem foi descoberto que de 4593 contas totais houveram 7882 mensagens não repetidas de **16 idiomas** diferentes vindas de 57 paises, num total de 1112 cidades e de 4593 latitudes diferentes. Enquanto o esperado era ter apenas um 1 idioma no total.

## Primeira Correção: remoção de idiomas não "pt"
"""

csv_para_filtrar = pd.read_csv("/content/DadosOrdemData.csv")

# Fonte: https://medium.com/@harsz89/how-to-drop-rows-based-on-column-values-using-pandas-dataframe-38cf50e4c95a
# Get indexes where name column doesn't have value john
indexNames = csv_para_filtrar[~(csv_para_filtrar['idioma'] == 'pt')].index 
# Delete these row indexes from dataFrame
csv_para_filtrar.drop(indexNames , inplace=True)

numero_lugares_unicos_filtrado = len(csv_para_filtrar.filter(items=['lugar','apelido']).groupby('lugar'))
numero_contas_unicas_filtrado = len(csv_para_filtrar.filter(items=['lugar','apelido']).groupby('apelido'))
numero_msg_unicas_filtrado = len(csv_para_filtrar.filter(items=['resumo','apelido']).groupby('resumo'))
numero_paises_unicos_filtrado = len(csv_para_filtrar.filter(items=['pais','apelido']).groupby('pais'))
numero_locacoes_unicos_filtrado = len(csv_para_filtrar.filter(items=['latitude','apelido']).groupby('apelido'))
numero_idioma_unicos_filtrado = len(csv_para_filtrar.filter(items=['idioma','apelido']).groupby('idioma'))

print("\n De",numero_contas_unicas_filtrado,"contas totais",
      "\n ha",numero_msg_unicas_filtrado,"mensagens não repetidas",
      "\n de",numero_idioma_unicos_filtrado,"idiomas diferentes",
      "\n de",numero_paises_unicos_filtrado,"paises",
      "\n e",numero_lugares_unicos_filtrado,"cidades",
      "\n e de",numero_locacoes_unicos_filtrado,"latitudes diferentes")

"""#### Dataframe somente idioma em pt"""

csv_para_filtrar = pd.read_csv("/content/DadosOrdemData.csv")

# Fonte: https://medium.com/@harsz89/how-to-drop-rows-based-on-column-values-using-pandas-dataframe-38cf50e4c95a
# Get indexes where name column doesn't have value john
indexNames = csv_para_filtrar[~(csv_para_filtrar['idioma'] == 'pt')].index 
# Delete these row indexes from dataFrame
csv_para_filtrar.drop(indexNames , inplace=True)

pd.DataFrame(csv_para_filtrar)

"""#### Dataframe somente idioma não pt"""

csv_para_filtrar = pd.read_csv("/content/DadosOrdemData.csv")

# Fonte: https://medium.com/@harsz89/how-to-drop-rows-based-on-column-values-using-pandas-dataframe-38cf50e4c95a
# Get indexes where name column doesn't have value john
indexNames = csv_para_filtrar[~(csv_para_filtrar['idioma'] != 'pt')].index 
# Delete these row indexes from dataFrame
csv_para_filtrar.drop(indexNames , inplace=True)

pd.DataFrame(csv_para_filtrar)

"""Feita a correção, através da análise dos idiomas em português é percepítvel que o dado usado se encontra igualmente falho. Existem frases em espanhol ou italiano que por serem de escrita semelhante foram categorizados como em português.

## Filtrando Perguntas

### Lógica utilizada
"""

# Tendo um datasheet escolhido
csv_utilizado = pd.read_csv("/content/DadosOrdemData.csv")
# É simplificado o texto colocando tudo em caixa baixa
filtro1 = csv_utilizado.iloc[48]['texto'].lower()
# Para então buscado a interrogação e adicionando espaços ao redor dela
filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
# As hashtags são removidas e substituídas por espaços
filtro3 = filtro2.replace("#"," ") # remove hashtags
# as quebras de linhas são substituidas por espaço
filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
# a acentuação do i é removido para evitar conflitos como em vírus x virus
filtro5 = filtro4.replace("í","i") # remove quebra de linha
# e a string é transformada em um array de caracteres tendo espaço como divisor
palavras = filtro5.split(' ')

# Essa lógica desenvolvida foi então colocada dentro do método de filtragem utilizado
print(palavras)

#@title Opções de Filtragem: prefeito { form-width: "70%" }
qual_palavra = "coronavirus" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_prefeito" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosNaoNuloFrmtApelido.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: Perguntas { form-width: "70%" }
qual_palavra = "?" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_interrogacao" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar: pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

csv_interrogacao = pd.read_csv("/content/pesquisa_interrogacao.csv")
pd.DataFrame(csv_interrogacao)



"""## Quantitativo de Palavras com "?""""

#@title Mapa Calor
url_para_uso = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

#@title Opções de Filtragem: presidente { form-width: "70%" }
qual_palavra = "presidente" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_presidente" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: governador { form-width: "70%" }
qual_palavra = "governador" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_governador" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: prefeito { form-width: "70%" }
qual_palavra = "prefeito" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_prefeito" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: morte { form-width: "70%" }
qual_palavra = "morte" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_morte" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: morrer { form-width: "70%" }
qual_palavra = "morrer" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_morrer" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: hospital { form-width: "70%" }
qual_palavra = "hospital" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_hospital" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: ministro { form-width: "70%" }
qual_palavra = "ministro" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_ministro" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: china { form-width: "70%" }
qual_palavra = "china" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "inter_china" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: chineses { form-width: "70%" }
qual_palavra = "chineses" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_chineses" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: bolsonaro { form-width: "70%" }
qual_palavra = "bolsonaro" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: vírus { form-width: "70%" }
qual_palavra = "\"virus\"" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_virus" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: gripezinha { form-width: "70%" }
qual_palavra = "gripezinha" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_gripezinha" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: coronavirus { form-width: "70%" }
qual_palavra = "coronavirus" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_coronavirus" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: pandemia { form-width: "70%" }
qual_palavra = "pandemia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_pandemia" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: mp { form-width: "70%" }
qual_palavra = "mp" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "inter_mp" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

"""## Quantitativo de Palavras Independente do "?""""

#@title Mapa Calor
url_para_uso = "/content/DadosOrdemData.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

#@title Opções de Filtragem: presidente { form-width: "70%" }
qual_palavra = "presidente" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: governador { form-width: "70%" }
qual_palavra = "governador" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: prefeito { form-width: "70%" }
qual_palavra = "prefeito" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: morte { form-width: "70%" }
qual_palavra = "morte" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: morrer { form-width: "70%" }
qual_palavra = "morrer" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: hospital { form-width: "70%" }
qual_palavra = "hospital" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: ministro { form-width: "70%" }
qual_palavra = "ministro" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: china { form-width: "70%" }
qual_palavra = "china" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: chineses { form-width: "70%" }
qual_palavra = "chineses" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: bolsonaro { form-width: "70%" }
qual_palavra = "bolsonaro" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: vírus { form-width: "70%" }
qual_palavra = "\"virus\"" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: gripezinha { form-width: "70%" }
qual_palavra = "gripezinha" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: coronavirus { form-width: "70%" }
qual_palavra = "coronavirus" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: pandemia { form-width: "70%" }
qual_palavra = "pandemia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: mp { form-width: "70%" }
qual_palavra = "mp" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

"""## Comparação de Quantitativos com "?"

### Presidente
"""

item = 'presidente'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 2
com_e_sem_interrogacao = 48

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Governador"""

item = 'governador'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 0
com_e_sem_interrogacao = 33

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Prefeito"""

item = 'prefeito'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 1
com_e_sem_interrogacao = 49

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Morte"""

item = 'morte'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 0
com_e_sem_interrogacao = 0

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Morrer"""

item = 'morrer'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 1
com_e_sem_interrogacao = 4

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Hospital"""

item = 'hospital'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 0
com_e_sem_interrogacao = 71

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Ministro"""

item = 'ministro'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 1
com_e_sem_interrogacao = 1

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### China"""

item = 'china'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 4
com_e_sem_interrogacao = 45

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Chineses"""

item = 'chineses'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 1
com_e_sem_interrogacao = 4

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Bolsonaro"""

item = 'bolsonaro'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 4
com_e_sem_interrogacao = 40

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Virus"""

item = 'virus'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 0
com_e_sem_interrogacao = 0

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Coronavírus"""

item = 'coronavirus'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 75
com_e_sem_interrogacao = 1327

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Gripezinha"""

item = 'gripezinha'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 0
com_e_sem_interrogacao = 3

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### Pandemia"""

item = 'pandemia'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 58
com_e_sem_interrogacao = 776

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""### MP"""

item = 'mp'
labels = 'Total Restante', 'Com ?', 'Sem ?'
total = 7777
com_interrogacao = 1
com_e_sem_interrogacao = 25

sections = [ (total - com_e_sem_interrogacao) , com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = ['c', 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0, 0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total de mensagens')
plt.show()


labels =  'Total com ?', 'Total sem ?'
sections = [ com_interrogacao, (com_e_sem_interrogacao - com_interrogacao)]
colors = [ 'g', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=0,
        explode = (0.1, .50),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('comparado ao total com a palavra ' + item)
plt.show()

"""## Segunda Conclusão da Interpretação

Através das primeiras execuções do código foi perceptível que o resultado eram valores muito abaixo do esperado e que revisões eram necessárias, melhorias foram feitas para resolver problemas como case sensitive e o espaçamento do foco que era a interrogação presente na frase.

## Pesquisas adicionais

Devido a conclusão obtida com a segunda interpretação, novas pesquisas foram feitas para validar o tipo de conteúdo presente e chegar a uma terceira conclusão.
"""

#@title Opções de Filtragem: praia { form-width: "70%" }
qual_palavra = "praia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: saudades { form-width: "70%" }
qual_palavra = "praia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: #TBT { form-width: "70%" }
qual_palavra = "tbt" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Opções de Filtragem: manaus { form-width: "70%" }
qual_palavra = "praia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

csv_utilizado = pd.read_csv("/content/pesquisa_interrogacao.csv")
# É simplificado o texto colocando tudo em caixa baixa
filtro1 = csv_utilizado.iloc[504]['texto'].lower()
# Para então buscado a interrogação e adicionando espaços ao redor dela
filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
# As hashtags são removidas e substituídas por espaços
filtro3 = filtro2.replace("#"," ") # remove hashtags
# as quebras de linhas são substituidas por espaço
filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
# a acentuação do i é removido para evitar conflitos como em vírus x virus
filtro5 = filtro4.replace("í","i") # remove quebra de linha
# e a string é transformada em um array de caracteres tendo espaço como divisor
palavras = filtro5.split(' ')


print(csv_utilizado.iloc[504]['apelido'].lower(),'\n digitou',
      'no dia',csv_utilizado.iloc[504]['data'].lower(),'\n',
      filtro5)

csv_utilizado = pd.read_csv("/content/pesquisa_interrogacao.csv")
# É simplificado o texto colocando tudo em caixa baixa
filtro1 = csv_utilizado.iloc[579]['texto'].lower()
# Para então buscado a interrogação e adicionando espaços ao redor dela
filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
# As hashtags são removidas e substituídas por espaços
filtro3 = filtro2.replace("#"," ") # remove hashtags
# as quebras de linhas são substituidas por espaço
filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
# a acentuação do i é removido para evitar conflitos como em vírus x virus
filtro5 = filtro4.replace("í","i") # remove quebra de linha
# e a string é transformada em um array de caracteres tendo espaço como divisor
palavras = filtro5.split(' ')

print(csv_utilizado.iloc[579]['apelido'].lower(),'\n digitou',
      'no dia',csv_utilizado.iloc[579]['data'].lower(),'\n',
      filtro5)

"""## Terceira Conclusão da Interpretação



Mesmo com as filtragens e a remoção de duplicatas, graças aos dados baixos obtidos com a pesquisa comparacional de palavras com ou sem interrogação, foi possível dizer que existe spam na datasheet trabalhado.

## Frequência das perguntas

A quarta interpretação se deu através da análise da frequência de perguntas por data e para isso foi criado uma nova coluna com a divisão da data e da hora para a montagem do gráfico.
"""

csv_utilizado = pd.read_csv("/content/pesquisa_interrogacao.csv")

new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
csv_utilizado['data'] = new[0].str.replace('-','/')
csv_utilizado['hora'] = new[1]

pd.DataFrame(csv_utilizado).tail()

"""A análise dos dados de maior valor foram feitos em seguida através de um value_counts(), nesse ponto que se mostrou essencial a divisão do dia da hora para evitar valores repetidas de mesmo dia com horário diferente."""

gmaior = csv_utilizado['data'].value_counts()
gmaior

"""E o gráfico foi montado aplicando o .sort_index() ao resultado obtido na contagem para se ter a progressão por tempo."""

g = csv_utilizado['data'].value_counts().sort_values().sort_index()
plt.plot(g)

"""Através da análise foi possível perceber que no dia 04 de abril houveram 22 perguntas. Foi feita uma pesquisa no histórico de notícias da pandêmia do G1 (fonte: https://g1.globo.com/bemestar/coronavirus/noticia/2020/04/04/ultimas-noticias-de-coronavirus-de-4-de-abril.ghtml ) para se saber quais foram as príncipais notícias do dia e entre elas houve a primeira morte no Amapá. Quando pesquisado no banco de textos em português foram encontradas dois resultados."""

#@title Teste de Filtragem 04/04/2020 - Amapá { form-width: "70%" }
qual_palavra = "amap\xE1" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_amapa" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Texto encontrado
posicao_parar_procurar = "4378" #@param {type:"string"}
url_csv_para_usar = "/content/DadosOrdemData.csv" #@param {type:"string"}
def filtrar_palavra(url_csv,posicao_procurada_str):
  csv_utilizado = pd.read_csv(url_csv)
  # É simplificado o texto colocando tudo em caixa baixa
  posicao_procurada = int(posicao_procurada_str)
  filtro1 = csv_utilizado.iloc[posicao_procurada]['texto'].lower()
  # Para então buscado a interrogação e adicionando espaços ao redor dela
  filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
  # As hashtags são removidas e substituídas por espaços
  filtro3 = filtro2.replace("#"," ") # remove hashtags
  # as quebras de linhas são substituidas por espaço
  filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
  # a acentuação do i é removido para evitar conflitos como em vírus x virus
  filtro5 = filtro4.replace("í","i") # remove quebra de linha
  # e a string é transformada em um array de caracteres tendo espaço como divisor
  palavras = filtro5.split(' ')
  print(csv_utilizado.iloc[posicao_procurada]['apelido'].lower(),'\n digitou',
      'no dia',csv_utilizado.iloc[posicao_procurada]['data'].lower(),'\n',
      filtro5)
  
filtrar_palavra(url_csv_para_usar,posicao_parar_procurar)

"""Mesmo não tendo a mesma data, é percepítvel que existe a cobrança a figuras políticas sobre as situações em diferentes estados."""

#@title Montar gráfico frequência simples por csv
url_do_csv_para_usar = "/content/DadosOrdemData.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

grafico_frequencia(url_do_csv_para_usar)



#@title Filtragem "Deputado" { form-width: "70%" }
qual_palavra = "deputado" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_deputado" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência "Deputado"
url_do_csv_para_usar = "/content/pesquisa_deputado.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)
  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

"""## Quarta Conclusão da Interpretação

Sendo feita novamente a pesquisa no G1 sobre o dia foi possível encontrar novas palavras chaves, assim, podendo concluir a importância do uso da frequência como essêncial para determinar as palavras necessárias para o projeto.

### Correção a lógica da filtragem
"""

# Tendo um datasheet escolhido
csv_utilizado = pd.read_csv("/content/DadosOrdemData.csv")
# É simplificado o texto colocando tudo em caixa baixa
filtro1 = csv_utilizado.iloc[48]['texto'].lower()
# Para então buscado a interrogação e adicionando espaços ao redor dela
filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
# As hashtags são removidas e substituídas por espaços
filtro3 = filtro2.replace("#"," ") # remove hashtags
# as quebras de linhas são substituidas por espaço
filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
# a acentuação do i é removido para evitar conflitos como em vírus x virus
filtro5 = filtro4.replace("í","i") # remove quebra de linha
filtro6 = filtro5.replace('ã','a') # remove caractere especial
filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
filtro8 = filtro7.replace('ç','c') # remove caractere especial
filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
filtro10 = filtro9.replace('ê','e') # remove caractere especial
filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
# e a string é transformada em um array de caracteres tendo espaço como divisor
palavras = filtro11.split(' ')

# Essa lógica desenvolvida foi então colocada dentro do método de filtragem utilizado
print(palavras)

"""### Correção método de filtragem"""

def filtrar_e_exportar_v2(csv_usado,formatar,palavra,exportar,nome):      
  obj_para_download = []
  dic_para_download = {}

  # ----------------------
  # FILTRANDO DADOS DOWNLOAD
  # ----------------------
  num_dataset_filtrado = len(csv_usado)
  ids_filtrados = []
  num_filtrados = 0
  for i in range(num_dataset_filtrado):
    filtro1 = csv_usado.iloc[i]['texto'].lower()
    # Para então buscado a interrogação e adicionando espaços ao redor dela
    filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
    # As hashtags são removidas e substituídas por espaços
    filtro3 = filtro2.replace("#"," ") # remove hashtags
    # as quebras de linhas são substituidas por espaço
    filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
    # a acentuação do i é removido para evitar conflitos como em vírus x virus
    filtro5 = filtro4.replace("í","i") # remove quebra de linha
    filtro6 = filtro5.replace('ã','a') # remove caractere especial
    filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
    filtro8 = filtro7.replace('ç','c') # remove caractere especial
    filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
    filtro10 = filtro9.replace('ê','e') # remove caractere especial
    filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
    # e a string é transformada em um array de caracteres tendo espaço como divisor
    palavras = filtro11.split(' ')
    adicionado = False

    if (i>=0):
    # if (formatar):
    #   edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})
    #   filtro1 = edit_filtro1
      for d in range(len(palavras)):
        if (d==0):
          adicionado = False #debug
        if (palavras[d]==palavra):
          # print('achou valor')
          num_filtrados +=1
          if (adicionado == False):
            adicionado = True
            ids_filtrados.append(csv_usado.iloc[i]['posicao'])
      # try:
      #   valor = filtro1.find(palavra)
      #   if (valor!=-1):
      #     num_filtrados +=1
      #     ids_filtrados.append(i)
      # except:
      #   print("erro")
      if (i == num_dataset_filtrado-1):
        print("\n Filtragem da palavra",palavra,"terminou",
              "\n Foram filtrados um total de", len(ids_filtrados), "itens",
              "\n sendo eles",ids_filtrados)
      
  # ----------------------
  # FORMATAR DOWNLOAD
  # ----------------------

  for i in range(len(ids_filtrados)):
      posicao_formatado = ids_filtrados[i]
      data_list = {'id': str(i),
                  'apelido':csv_usado.iloc[posicao_formatado]['apelido'],
                  'texto':csv_usado.iloc[posicao_formatado]['texto'],
                  'data':csv_usado.iloc[posicao_formatado]['data'],
                  'idioma':csv_usado.iloc[posicao_formatado]['idioma'],
                  'pais':csv_usado.iloc[posicao_formatado]['pais'],
                  'latitude':csv_usado.iloc[posicao_formatado]['latitude'],
                  'longitude':csv_usado.iloc[posicao_formatado]['longitude'],
                  'sigla':csv_usado.iloc[posicao_formatado]['sigla'],
                  'lugar':csv_usado.iloc[posicao_formatado]['lugar']}
      obj_para_download.append(data_list)

  # ----------------------
  # EXPORTANDO CSV
  # ----------------------

  gerarDownload = exportar
  nome_csv = "datasheet_" + str(palavra)
  if(nome != ""):
    nome_csv = nome_do_arquivo

  if(gerarDownload):
    for di in obj_para_download:
      dic_para_download[di['id']]={}
      for k in di.keys():
        if k =='id': continue
        dic_para_download[di['id']][k]=di[k]
        
    with open( nome_csv +'.csv', 'w') as csv_file:  
      writer = csv.writer(csv_file)
      writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                      'idioma','pais','latitude','longitude','sigla',
                      'lugar','resumo'])
      for key, value in dic_para_download.items():
        writer.writerow([key, value['apelido'], value['data'], 
                        value['texto'], value['idioma'],
                        value['pais'], value['latitude'],
                        value['longitude'], value['sigla'],
                        value['lugar'] ])

def imprimir_csv(url_para_imprimir):
  csv_passado = pd.read_csv(url_para_imprimir)
  return pd.DataFrame(csv_passado)

#@title Opções de Filtragem Versão 2: pandemia { form-width: "70%" }
qual_palavra = "pandemia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar_v2(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

"""Com as correções feitas a pesquisa por pandêmia foi de 776 itens para 986 em todos os dados em português."""

#@title Opções de Filtragem Versão 2: pandemia com interrogação { form-width: "70%" }
qual_palavra = "pandemia" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = False #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_oportunidades" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar_v2(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

"""Com as correções feitas a pesquisa por pandêmia foi de 58 itens para 72 em todos os dados em português.

### Criação de novo datasheet de perguntas
"""

#@title Opções de Filtragem Versão 2: pandemia com interrogação { form-width: "70%" }
qual_palavra = "?" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_interrogacao_v2" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

filtrar_e_exportar_v2(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência Pergunta v2
url_do_csv_para_usar = "/content/pesquisa_interrogacao_v2.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)
  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Mapa interativo
url_para_uso = "/content/pesquisa_interrogacao_v2.csv" #@param {type:"string"}
def generate_map(url_do_csv):
  m = folium.Map(
      height='80%',
      width='80%',
      location=[-23.53, -46.79],
      zoom_start=4,
      tiles='Stamen Terrain'
  )
  csv_para_usar = pd.read_csv(url_do_csv)
  for i in range(len(csv_para_usar)):
    if (i < 500):
      filtro1 = csv_utilizado.iloc[i]['texto'].lower()
      # Para então buscado a interrogação e adicionando espaços ao redor dela
      filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
      # As hashtags são removidas e substituídas por espaços
      filtro3 = filtro2.replace("#"," ") # remove hashtags
      # as quebras de linhas são substituidas por espaço
      filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
      # a acentuação do i é removido para evitar conflitos como em vírus x virus
      filtro5 = filtro4.replace("í","i") # remove quebra de linha
      filtro6 = filtro5.replace('ã','a') # remove caractere especial
      filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
      filtro8 = filtro7.replace('ç','c') # remove caractere especial
      filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
      filtro10 = filtro9.replace('ê','e') # remove caractere especial
      filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
      filtro12 = filtro11.replace('á', 'a') # ajusta espaçamento de palavra
      filtro13 = filtro12.replace('à', 'a') # ajusta espaçamento de palavra
      tooltip = csv_utilizado.iloc[i]['apelido'].lower()
      texto = '<i>' +  csv_utilizado.iloc[i]['data'].lower() + " em " + csv_utilizado.iloc[i]['lugar'].lower() + " - " + filtro11 +' </i>'
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      folium.Marker([float(longitude),float(latitude)], popup=texto, tooltip=tooltip).add_to(m)
  
  return m

generate_map(url_para_uso)

"""### Método de filtragem de Array de Palavras"""

def filtrar_array_e_exportar_(csv_usado,formatar,palavra,exportar,nome):      
  obj_para_download = []
  dic_para_download = {}

  # ----------------------
  # FILTRANDO DADOS DOWNLOAD
  # ----------------------
  num_dataset_filtrado = len(csv_usado)
  ids_filtrados = []
  num_filtrados = 0
  for i in range(num_dataset_filtrado):
    filtro1 = csv_usado.iloc[i]['texto'].lower()
    # Para então buscado a interrogação e adicionando espaços ao redor dela
    filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
    # As hashtags são removidas e substituídas por espaços
    filtro3 = filtro2.replace("#"," ") # remove hashtags
    # as quebras de linhas são substituidas por espaço
    filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
    # a acentuação do i é removido para evitar conflitos como em vírus x virus
    filtro5 = filtro4.replace("í","i") # remove quebra de linha
    filtro6 = filtro5.replace('ã','a') # remove caractere especial
    filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
    filtro8 = filtro7.replace('ç','c') # remove caractere especial
    filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
    filtro10 = filtro9.replace('ê','e') # remove caractere especial
    filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
    # e a string é transformada em um array de caracteres tendo espaço como divisor
    palavras = filtro11.split(' ')
    adicionado = False

    if (i>=0):
    # if (formatar):
    #   edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})
    #   filtro1 = edit_filtro1
      for d in range(len(palavras)):
        if (d==0):
          adicionado = False #debug
        for e in range(len(palavra)):
          if (palavras[d]==palavra[e]):
            # print('achou valor')
            num_filtrados +=1
            if (adicionado == False):
              adicionado = True
              ids_filtrados.append(csv_usado.iloc[i]['posicao'])
      # try:
      #   valor = filtro1.find(palavra)
      #   if (valor!=-1):
      #     num_filtrados +=1
      #     ids_filtrados.append(i)
      # except:
      #   print("erro")
      if (i == num_dataset_filtrado-1):
        print("\n Filtragem da palavra",palavra,"terminou",
              "\n Foram filtrados um total de", len(ids_filtrados), "itens",
              "\n sendo eles",ids_filtrados)
      
  # ----------------------
  # FORMATAR DOWNLOAD
  # ----------------------

  for i in range(len(ids_filtrados)):
      posicao_formatado = ids_filtrados[i]
      data_list = {'id': str(i),
                  'apelido':csv_usado.iloc[posicao_formatado]['apelido'],
                  'texto':csv_usado.iloc[posicao_formatado]['texto'],
                  'data':csv_usado.iloc[posicao_formatado]['data'],
                  'idioma':csv_usado.iloc[posicao_formatado]['idioma'],
                  'pais':csv_usado.iloc[posicao_formatado]['pais'],
                  'latitude':csv_usado.iloc[posicao_formatado]['latitude'],
                  'longitude':csv_usado.iloc[posicao_formatado]['longitude'],
                  'sigla':csv_usado.iloc[posicao_formatado]['sigla'],
                  'lugar':csv_usado.iloc[posicao_formatado]['lugar']}
      obj_para_download.append(data_list)

  # ----------------------
  # EXPORTANDO CSV
  # ----------------------

  gerarDownload = exportar
  nome_csv = "datasheet_" + str(palavra)
  if(nome != ""):
    nome_csv = nome_do_arquivo

  if(gerarDownload):
    for di in obj_para_download:
      dic_para_download[di['id']]={}
      for k in di.keys():
        if k =='id': continue
        dic_para_download[di['id']][k]=di[k]
        
    with open( nome_csv +'.csv', 'w') as csv_file:  
      writer = csv.writer(csv_file)
      writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                      'idioma','pais','latitude','longitude','sigla',
                      'lugar','resumo'])
      for key, value in dic_para_download.items():
        writer.writerow([key, value['apelido'], value['data'], 
                        value['texto'], value['idioma'],
                        value['pais'], value['latitude'],
                        value['longitude'], value['sigla'],
                        value['lugar'] ])

"""# Pesquisa quanto a pessoa"""

qual_palavra = "presidente,governador,prefeito,deputado,bolso,bolsonaro,doria,mandetta,mandeta,moro,ministro,gov,min,hulk,luciano,pink,charles,elizabeth,boulos,ciro,lula,trump"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_pessoas_interrogacao_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao_v2.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Imprimir data_sheet
url_para_imprimir = "/content/pesquisa_pessoas_interrogacao_1.csv" #@param {type:"string"}
imprimir_csv(url_para_imprimir)

qual_palavra = "presidente,governador,prefeito,deputado,bolso,bolsonaro,doria,mandetta,mandeta,moro,ministro,gov,min,hulk,luciano,pink,charles,elizabeth,boulos,ciro,lula,trump"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_pessoas_geral_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência Pessoas
url_do_csv_para_usar = "/content/pesquisa_pessoas_geral_1.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Mapa Calor
url_para_uso = "/content/pesquisa_pessoas_geral_1.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

"""# Pesquisa quanto a doença"""

qual_palavra = "coronavirus,covid-19,covid,virus,gripezinha"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_coronavirus_interrogacao_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao_v2.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Imprimir data_sheet
url_para_imprimir = "/content/pesquisa_coronavirus_interrogacao_1.csv" #@param {type:"string"}
imprimir_csv(url_para_imprimir)

qual_palavra = "coronavirus,covid-19,covid,virus,gripezinha"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_coronavirus_geral_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência Doença
url_do_csv_para_usar = "/content/pesquisa_coronavirus_geral_1.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Mapa Calor
url_para_uso = "/content/pesquisa_coronavirus_geral_1.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

"""# Pesquisa quanto a medicamento"""

qual_palavra = "cloroquina,cha,vitamina,zinco,inhame,pr\xF3polis,erva,ch\xE1,hidratacao,sopa,abacaxi"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_medicamento_interrogacao_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao_v2.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Imprimir data_sheet
url_para_imprimir = "/content/pesquisa_medicamento_interrogacao_1.csv" #@param {type:"string"}
imprimir_csv(url_para_imprimir)

qual_palavra = "cloroquina,cha,vitamina,zinco,inhame,pr\xF3polis,erva,ch\xE1,hidratacao,sopa,abacaxi"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_medicamento_geral_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência Medicamento
url_do_csv_para_usar = "/content/pesquisa_medicamento_geral_1.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Mapa Calor
url_para_uso = "/content/pesquisa_medicamento_geral_1.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

"""# Pesquisa quanto a organizações"""

qual_palavra = "prefeitura,delegacia,oms,organizacao,mundial,saude,brics,caixa"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_organizacoes_interrogacao_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/pesquisa_interrogacao_v2.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Imprimir data_sheet
url_para_imprimir = "/content/pesquisa_organizacoes_interrogacao_1.csv" #@param {type:"string"}
imprimir_csv(url_para_imprimir)

qual_palavra = "prefeitura,delegacia,oms,organizacao,mundial,saude,brics,caixa"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_organizacoes_geral_1" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/DadosOrdemData.csv" #@param ["/content/pesquisa_interrogacao_v2.csv", "/content/DadosOrdemData.csv"]
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Gráfico Frequência Organização
url_do_csv_para_usar = "/content/pesquisa_organizacoes_geral_1.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Mapa Calor
url_para_uso = "/content/pesquisa_organizacoes_geral_1.csv" #@param {type:"string"}

def generate_mapa_calor():
  data = []
  for i in range(len(csv_para_usar)):
    if (i < 500):
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      data.append([float(longitude),float(latitude)])
  m = folium.Map(tiles='stamentoner', 
      height='80%',
      width='50%',
      location=[-23.53, -46.79],
      zoom_start=4)
  HeatMap(data).add_to(m)
  #m.save(os.path.join('results', 'Heatmap.html'))
  return m

generate_mapa_calor()

"""# Dados sem Localização"""

#@title Opções de Filtragem Sem Lugar: perguntas com interrogação { form-width: "70%" }


def filtrar_e_exportar_v3(csv_usado,formatar,palavra,exportar,nome):      
  obj_para_download = []
  dic_para_download = {}

  # ----------------------
  # FILTRANDO DADOS DOWNLOAD
  # ----------------------
  df2 = csv_usado.dropna(subset=['lugar'])
  df1 = csv_usado.drop(df2.index)
  csv_usado = df1

  # Fonte: https://medium.com/@harsz89/how-to-drop-rows-based-on-column-values-using-pandas-dataframe-38cf50e4c95a
  # Get indexes where name column doesn't have value john
  indexNames = df1[~(df1['idioma'] == 'pt')].index 
  # Delete these row indexes from dataFrame
  df1.drop(indexNames , inplace=True)
  num_dataset_filtrado = len(df1)
  ids_filtrados = []
  num_filtrados = 0
  for i in range(num_dataset_filtrado):
    filtro1 = csv_usado.iloc[i]['texto'].lower()
    # Para então buscado a interrogação e adicionando espaços ao redor dela
    filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
    # As hashtags são removidas e substituídas por espaços
    filtro3 = filtro2.replace("#"," ") # remove hashtags
    # as quebras de linhas são substituidas por espaço
    filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
    # a acentuação do i é removido para evitar conflitos como em vírus x virus
    filtro5 = filtro4.replace("í","i") # remove quebra de linha
    filtro6 = filtro5.replace('ã','a') # remove caractere especial
    filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
    filtro8 = filtro7.replace('ç','c') # remove caractere especial
    filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
    filtro10 = filtro9.replace('ê','e') # remove caractere especial
    filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
    # e a string é transformada em um array de caracteres tendo espaço como divisor
    palavras = filtro11.split(' ')
    adicionado = False

    if (i>=0):
    # if (formatar):
    #   edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})
    #   filtro1 = edit_filtro1
      for d in range(len(palavras)):
        if (d==0):
          adicionado = False #debug
        if (palavras[d]==palavra):
          # print('achou valor')
          num_filtrados +=1
          if (adicionado == False):
            adicionado = True
            ids_filtrados.append(i)
      # try:
      #   valor = filtro1.find(palavra)
      #   if (valor!=-1):
      #     num_filtrados +=1
      #     ids_filtrados.append(i)
      # except:
      #   print("erro")
      if (i == num_dataset_filtrado-1):
        print("\n Filtragem da palavra",palavra,"terminou",
              "\n Foram filtrados um total de", len(ids_filtrados), "itens",
              "\n sendo eles",ids_filtrados)
      
  # ----------------------
  # FORMATAR DOWNLOAD
  # ----------------------

  for i in range(len(ids_filtrados)):
      posicao_formatado = ids_filtrados[i]
      data_list = {'id': str(i),
                  'apelido':csv_usado.iloc[posicao_formatado]['apelido'],
                  'texto':csv_usado.iloc[posicao_formatado]['texto'],
                  'data':csv_usado.iloc[posicao_formatado]['data'],
                  'idioma':csv_usado.iloc[posicao_formatado]['idioma'],
                  'pais':csv_usado.iloc[posicao_formatado]['pais'],
                  'latitude':csv_usado.iloc[posicao_formatado]['latitude'],
                  'longitude':csv_usado.iloc[posicao_formatado]['longitude'],
                  'sigla':csv_usado.iloc[posicao_formatado]['sigla'],
                  'lugar':csv_usado.iloc[posicao_formatado]['lugar']}
      obj_para_download.append(data_list)

  # ----------------------
  # EXPORTANDO CSV
  # ----------------------

  gerarDownload = exportar
  nome_csv = "datasheet_" + str(palavra)
  if(nome != ""):
    nome_csv = nome_do_arquivo

  if(gerarDownload):
    for di in obj_para_download:
      dic_para_download[di['id']]={}
      for k in di.keys():
        if k =='id': continue
        dic_para_download[di['id']][k]=di[k]
        
    with open( nome_csv +'.csv', 'w') as csv_file:  
      writer = csv.writer(csv_file)
      writer.writerow(['posicao', 'apelido', 'data', 'texto', 
                      'idioma','pais','latitude','longitude','sigla',
                      'lugar','resumo'])
      for key, value in dic_para_download.items():
        writer.writerow([key, value['apelido'], value['data'], 
                        value['texto'], value['idioma'],
                        value['pais'], value['latitude'],
                        value['longitude'], value['sigla'],
                        value['lugar'] ])
        

qual_palavra = "?" #@param {type:"string"}
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_interrogacao_v4" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/drive/My Drive/dados-curso-completo.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")
copilar = False #@param {type:"boolean"}

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

if (copilar == True):
  filtrar_e_exportar_v3(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)



#@title Gráfico Frequência Perguntas com Interrogação Sem Lugar
url_do_csv_para_usar = "/content/drive/My Drive/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Pesquisa quanto a pessoas

qual_palavra = "presidente,governador,prefeito,deputado,bolso,bolsonaro,doria,mandetta,mandeta,moro,ministro,gov,min,hulk,luciano,pink,charles,elizabeth,boulos,ciro,lula,trump"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_pessoas_sem_local" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/drive/My Drive/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Pesquisa quanto a doença

qual_palavra = "coronavirus,covid-19,covid,virus,gripezinha"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_coronavirus_sem_local" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/drive/My Drive/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Pesquisa quanto a organizações

qual_palavra = "prefeitura,delegacia,oms,organizacao,mundial,saude,brics,caixa"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_organizacoes_sem_local" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/drive/My Drive/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

#@title Pesquisa quanto a medicamento

qual_palavra = "cloroquina,cha,vitamina,zinco,inhame,pr\xF3polis,erva,ch\xE1,hidratacao,sopa,abacaxi"  #@param {type:"string"}
quais_palavras = qual_palavra.split(",")
formatar_texto = False #@param {type:"boolean"}
exportar_csv = True #@param {type:"boolean"}
nome_do_arquivo = "pesquisa_medicamento_sem_local" #@param {type:"string"}
usar_csv_url = True #@param {type:"boolean"}
url_do_csv = "/content/drive/My Drive/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
csv_para_usar = pd.read_csv("https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/DadosOrdemData.csv")

if (usar_csv_url):
  csv_para_usar = pd.read_csv(url_do_csv)

#print(quais_palavras)
filtrar_array_e_exportar_(csv_para_usar,False,quais_palavras,exportar_csv,nome_do_arquivo)

"""# Conclusões"""

#@title Visão Geral
labels = 'Com Localizacao Sem Interrogacao', 'Com Localizacao Com Interrogacao', 'Sem Localizacao Com Interrogacao', 'Spam e Duplicados'

local_sem_int = 7185
sem_local_int = 592
com_local_int = 159589
total = 1658825

valor1 = local_sem_int
valor2 = sem_local_int
valor3 = com_local_int

valor4 = total - (local_sem_int + com_local_int + sem_local_int )
sections = [valor1, valor2, valor3, valor4]
colors = ['g', 'r', 'y', 'c']

plt.pie(sections, labels=labels, colors=colors,
        startangle=180,
        explode = (0.8, 0.4, 0.2, 0.4),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('Total de Dados \n')
plt.show()

#@title Utilisado

labels = 'Com Localizacao Sem Interrogacao', 'Com Localizacao Com Interrogacao', 'Sem Localizacao Com Interrogacao'

local_sem_int = 7185
sem_local_int = 592
com_local_int = 159589

valor1 = local_sem_int
valor2 = sem_local_int
valor3 = com_local_int

sections = [valor1, valor2, valor3]
colors = ['g', 'r', 'y']

plt.pie(sections, labels=labels, colors=colors,
        startangle=180,
        explode = (0.8, 0.4, 0.2),
        autopct = '%1.2f%%')

plt.axis('equal') # Try commenting this out.
plt.title('Total de Dados \n')
plt.show()

"""## Sobre o processo

Quanto aos dados analisados, primeiro foi dado atenção a sua localização para a construção de um mapa e uma interface que facilita-se a leitura, no processo dessa construção a língua portuguesa e todas as suas nuanças se tornaram o maior obstáculos e a formatação da frase para sua interpretação se tornou um processo de constantes revisões que resultaram na compreensão do conteúdo presente e seus spams (citações, poemas, propagandas, assuntos diversos que fogem da proposta). Quando analisado os dados sem localização, jornais, reportagens e ciberataques de com perguntas redundantes se tornaram mais visíveis. Contudo, frases semelhantes porém variando em gênero e número não foram removidas no processo de duplicatas por falta de tempo e conhecimento técnico.
"""

#@title Mapa interativo
url_para_uso = "/content/pesquisa_interrogacao_v2.csv" #@param {type:"string"}
def generate_map(url_do_csv):
  m = folium.Map(
      height='80%',
      width='80%',
      location=[-23.53, -46.79],
      zoom_start=4,
      tiles='Stamen Terrain'
  )
  csv_para_usar = pd.read_csv(url_do_csv)
  for i in range(len(csv_para_usar)):
    if (i < 500):
      filtro1 = csv_utilizado.iloc[i]['texto'].lower()
      # Para então buscado a interrogação e adicionando espaços ao redor dela
      filtro2 = filtro1.replace("?"," ? ") # assim, deixando a interrogacao mais visivel
      # As hashtags são removidas e substituídas por espaços
      filtro3 = filtro2.replace("#"," ") # remove hashtags
      # as quebras de linhas são substituidas por espaço
      filtro4 = filtro3.replace("\n"," ") # remove quebra de linha
      # a acentuação do i é removido para evitar conflitos como em vírus x virus
      filtro5 = filtro4.replace("í","i") # remove quebra de linha
      filtro6 = filtro5.replace('ã','a') # remove caractere especial
      filtro7 = filtro6.replace('.',' . ') # ajusta espaçamento de palavra
      filtro8 = filtro7.replace('ç','c') # remove caractere especial
      filtro9 = filtro8.replace('!',' ! ') # remove caractere especial
      filtro10 = filtro9.replace('ê','e') # remove caractere especial
      filtro11 = filtro10.replace(',', ' ') # ajusta espaçamento de palavra
      filtro12 = filtro11.replace('á', 'a') # ajusta espaçamento de palavra
      filtro13 = filtro12.replace('à', 'a') # ajusta espaçamento de palavra
      tooltip = csv_utilizado.iloc[i]['apelido'].lower()
      texto = '<i>' +  csv_utilizado.iloc[i]['data'].lower() + " em " + csv_utilizado.iloc[i]['lugar'].lower() + " - " + filtro11 +' </i>'
      latitude = csv_utilizado.iloc[i]['latitude']
      longitude = csv_utilizado.iloc[i]['longitude']
      folium.Marker([float(longitude),float(latitude)], popup=texto, tooltip=tooltip).add_to(m)
  
  return m

generate_map(url_para_uso)

"""## Sobre a frequência de postagens na rede social

Pelos gráficos de frequência o período de Abril até Junho foram os mais ativos na rede social analisada, fatores para isso incluem o isolamento social e a quarenta sugerida por diversos estados, mas quando analisados as principais notícias dos dias de pico a atividade de cada datasheet filtrado pode se atribuir esse aumento a “famosos doentes”, “social influencers de direita comprando briga com o STF”, “quantitativo de infectados ou mortes na china pela doença” e “o aumento do número de mortos no 
país”.
"""

#@title Gráfico Frequência Postagem
url_do_csv_para_usar = "https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/datas_gerados/pesquisa_interrogacao_v4.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

"""## Sobre a doença

O nome da doença e suas variações foram os termos mais utilizados, porém isso não se reflete a qualidade do conteúdo onde as palavras estavam presentes nas frases. Contudo, os dias seguintes que se deram após o. começo de mortes em determinado estado foram os dias que houveram altas nas perguntas sobre prevenção a doença.
"""

#@title Gráfico Frequência Doença
url_do_csv_para_usar = "/content/pesquisa_coronavirus_geral_1.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

#@title Gráfico Frequência Doença
url_do_csv_para_usar = "/content/pesquisa_coronavirus_sem_local.csv" #@param {type:"string"}
def grafico_frequencia(url_csv):
  csv_passado = pd.read_csv(url_csv)
  csv_utilizado = csv_passado.copy()
  new = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  csv_utilizado['data'] = new[0].str.replace('-','/')
  csv_utilizado['hora'] = new[1]
  g = csv_utilizado['data'].value_counts().sort_index()
  plt.plot(g)

  # new2 = csv_utilizado["data"].str.split(" ",n=1,expand=True)
  # array_datafmt = new[0].str.split('-',n=2,expand=True)
  # csv_utilizado['mes'] = array_datafmt[1]
  # csv_utilizado['hora'] = new[1]
  # g1 = csv_utilizado['mes'].value_counts().sort_index()
  # print(g1)

  print(csv_utilizado['data'].value_counts(), '\n')

grafico_frequencia(url_do_csv_para_usar)

"""## Agradecimentos

Graças ao trabalho desenvolvido foi perceptível a dificuldade do trabalho de interpretação de  grandes volumes de dados e da importância que as ferramentas como colab, github e as bibliotecas Pandas e Matplotlib têm no auxilio desse processo como um todo. Os autores dessa pesquisa agradecem a oportunidade do curso de especialização em Ciência de Dados cedido pela Universidade Estadual do Amazonas (UEA) e do aprendizado obtido graças aos professores doutores Elloa Guedes e Tiago Mello.
"""