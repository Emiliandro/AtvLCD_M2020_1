{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trab_pos.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PsIzE9t96WIg",
        "SD3XSc_87DCw",
        "_x-UXVM6bIKB",
        "VbSg-A2EqWtX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xCqeIFR59sV"
      },
      "source": [
        "# Introdu√ß√£o ao Projeto\n",
        "\n",
        "No cen√°rio atual com o novo corona v√≠rus e a grande preocupa√ß√£o da popula√ß√£o mundial a respeito deste assunto, achou-se necess√°rio fazer uma an√°lise simples para por em pr√°tia o conte√∫do compartilhado em sala de aula.\n",
        "\n",
        "Objetivo:\n",
        "Separar as d√∫vidas/perguntas sobre o COVID-19 e observar quais tipos de questionamentos mais feito na rede social relacionado ao v√≠rus.\n",
        "Qual personalidade mais comentada na rede quando o assunto √© COVID-19. \n",
        "Se poss√≠vel qual a regi√£o que mais questiona sobre o v√≠rus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG7MPqlM58gA"
      },
      "source": [
        "import pandas as pd\n",
        "import array\n",
        "import datetime\n",
        "import operator\n",
        "import csv\n",
        "from google.colab import drive\n",
        "import string\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsIzE9t96WIg"
      },
      "source": [
        "# Prepara√ß√£o da Leitura dos Dados\n",
        "\n",
        "Devido ao tamanho do arquivo datasheet em .csv (350mb) disponibilizado pelo professor, dificuldades foram encontradas no come√ßo do projeto, dessa forma, tratar o arquivo foi o problema inicial. A primeira solu√ß√£o para o problema foi o uso do Google Drive e da lib Panda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqzF2eBBdjdZ"
      },
      "source": [
        "##### Vari√°veis usadas no projeto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_qdbOi5Co-"
      },
      "source": [
        "\n",
        "class Utils:\n",
        "  # v√°riaveis de projeto\n",
        "  strBRASIL     = \"Brazil\"\n",
        "  strBr         = \"BR\"\n",
        "  fmtPorData    = \"%d/%m/%Y\"\n",
        "  fmtPorHorario = \"%H:%M:%S\"\n",
        "  fmtPorMes     = \"%m\"\n",
        "  fmtPorDia     = \"%d\"\n",
        "\n",
        "  csvCompleto   = None\n",
        "  csvComLocal   = None\n",
        "  csvEmPt       = None\n",
        "  csvOrdemNome  = None\n",
        "  csvFiltragem1 = None\n",
        "  csvFiltragem2 = None\n",
        "  csvFiltragem3 = None\n",
        "  csvFiltragem4 = None\n",
        "\n",
        "\n",
        "  obj_para_download = []\n",
        "  dic_para_download = {}\n",
        "\n",
        "dados = Utils()\n",
        "# Abaixo os Arrays usados no projeto para montar os csvs\n",
        "dicTexto = []\n",
        "dicLugar = []\n",
        "dicPais = []\n",
        "dicData = []\n",
        "dicApelido = []\n",
        "dicLatitude = []\n",
        "dicLongitude = []\n",
        "dicIdioma = []\n",
        "dicSigla = []\n",
        "dicResumo = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5BAJildbf3"
      },
      "source": [
        "##### Adi√ß√£o de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvAWuWRe6ZsJ"
      },
      "source": [
        "#@title Carregar Dados Completos (Google Drive) { form-width: \"45%\" }\n",
        "\n",
        "montarDrive = False #@param {type:\"boolean\"}\n",
        "carregar_completos = False #@param {type:\"boolean\"}\n",
        "\n",
        "if (montarDrive): \n",
        "  drive.mount('/content/drive')\n",
        "  if(carregar_completos):\n",
        "    dados.csvCompleto = pd.read_csv(\"/content/drive/My Drive/dados-curso-completo.csv\")\n",
        "    dados.csvCompleto.dropna (inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qAgFZBucfTy"
      },
      "source": [
        "##### Metodos de suporte no desenvolvimento do projeto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opyaTTRlcXE5"
      },
      "source": [
        "def limparArrays():\n",
        "  dicTexto = []\n",
        "  dicLugar = []\n",
        "  dicPais = []\n",
        "  dicData = []\n",
        "  dicApelido = []\n",
        "  dicLatitude = []\n",
        "  dicLongitude = []\n",
        "  dicIdioma = []\n",
        "  dicSigla = []\n",
        "  dicResumo = []\n",
        "  print(\"limparArrays()\",\"Arrays Limpos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD3XSc_87DCw"
      },
      "source": [
        "# Leitura dos Dados\n",
        "\n",
        "Tendo o arquivo completo acess√≠vel, foi primeiro removido os dados nulos atrav√©s do uso da biblioteca panda e da fun√ß√£o dropna, assim o arquivo foi divido entre dois datasets \"com local\" e \"sem local\".\n",
        "\n",
        "Para a realiza√ß√£o desta tarefa, foi obrigat√≥rio o uso da linguagem de programa√ß√£o Python 3 (e\n",
        "superiores) e das bibliotecas Pandas e NumPy. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viDCNk36Rzkh"
      },
      "source": [
        "## Prepara√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxNT3uQve5cK"
      },
      "source": [
        "#### Remo√ß√£o de dados nulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFCWwF0E8MUQ",
        "outputId": "a0603c52-707b-405f-f62c-2308e88cf349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "dados.csvComLocal = pd.read_csv(\"https://github.com/Emiliandro/AtvLCD_M2020_1/blob/master/separando-csv/exports-csv/comlocal.csv?raw=true\")\n",
        "dados.csvComLocal.dropna (inplace=True)\n",
        "pd.DataFrame(dados.csvComLocal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,5,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data/0</th>\n",
              "      <th>data/1</th>\n",
              "      <th>data/2</th>\n",
              "      <th>data/3</th>\n",
              "      <th>data/4</th>\n",
              "      <th>data/5</th>\n",
              "      <th>data/6</th>\n",
              "      <th>data/7</th>\n",
              "      <th>data/8</th>\n",
              "      <th>data/9</th>\n",
              "      <th>data/10</th>\n",
              "      <th>data/11</th>\n",
              "      <th>data/12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data</td>\n",
              "      <td>usuario</td>\n",
              "      <td>apelido</td>\n",
              "      <td>texto</td>\n",
              "      <td>retweet</td>\n",
              "      <td>seguidores</td>\n",
              "      <td>idioma</td>\n",
              "      <td>lugar</td>\n",
              "      <td>pais</td>\n",
              "      <td>sigla</td>\n",
              "      <td>latitude</td>\n",
              "      <td>longitude</td>\n",
              "      <td>hashtags</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-19 17:49:40</td>\n",
              "      <td>Nova Impress√£o</td>\n",
              "      <td>novaimpressao_</td>\n",
              "      <td>üö®Todos juntos contra COVID-19üí™\\r\\n\\r\\nSolicite...</td>\n",
              "      <td>0</td>\n",
              "      <td>124</td>\n",
              "      <td>pt</td>\n",
              "      <td>Bras√≠lia</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-47.8778</td>\n",
              "      <td>-15.77691</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2020-04-16 00:34:54</td>\n",
              "      <td>Felipe Silva</td>\n",
              "      <td>felipesilvasr</td>\n",
              "      <td>A secretaria de Sa√∫de da Para√≠ba confirmou nes...</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>pt</td>\n",
              "      <td>Tapero√°</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-36.825</td>\n",
              "      <td>-7.16277778</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020-05-03 13:48:52</td>\n",
              "      <td>Bruno Fortes</td>\n",
              "      <td>Brunaogf</td>\n",
              "      <td>Doming√£o ta√≠! ‚òÄÔ∏è\\r\\nEm casa! üòêüò¨\\r\\nVai passar!...</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>pt</td>\n",
              "      <td>Fortaleza</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-38.5266</td>\n",
              "      <td>-3.7293</td>\n",
              "      <td>['emcasa', 'isolamentosocial', 'saude', 'quero...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2020-03-31 17:52:37</td>\n",
              "      <td>jairo santos souza</td>\n",
              "      <td>soi_jairo</td>\n",
              "      <td>Seguindo orienta√ß√£o do Presidente Bolsonaro, o...</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>pt</td>\n",
              "      <td>Euclides da Cunha</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-39.01462</td>\n",
              "      <td>-10.50525</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74569</th>\n",
              "      <td>2020-04-16 19:45:29</td>\n",
              "      <td>Bruno Agrofoglio Ferreira</td>\n",
              "      <td>agrofoglio</td>\n",
              "      <td>Criei um bot para o Telegram raspando os dados...</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>pt</td>\n",
              "      <td>Paul√≠nia</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-47.203</td>\n",
              "      <td>-22.7572</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74580</th>\n",
              "      <td>2020-05-31 12:38:32</td>\n",
              "      <td>BAR√ÉO DE CEARA MIRIM</td>\n",
              "      <td>baraocearamirim</td>\n",
              "      <td>Serra da Formiga em Riachuelo RN\\r\\nDestino ma...</td>\n",
              "      <td>0</td>\n",
              "      <td>808</td>\n",
              "      <td>pt</td>\n",
              "      <td>Riachuelo</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-35.8238</td>\n",
              "      <td>-5.81579</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74581</th>\n",
              "      <td>2020-03-23 23:48:19</td>\n",
              "      <td>Diego Pelizari</td>\n",
              "      <td>diegopelizari</td>\n",
              "      <td>#Repost de respeito da parceira agroschoolbras...</td>\n",
              "      <td>0</td>\n",
              "      <td>174</td>\n",
              "      <td>pt</td>\n",
              "      <td>S√£o F√©lix do Araguaia</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-53</td>\n",
              "      <td>-11</td>\n",
              "      <td>['Repost']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74596</th>\n",
              "      <td>2020-05-12 09:30:34</td>\n",
              "      <td>Hamilton Valerio</td>\n",
              "      <td>hvalerio</td>\n",
              "      <td>Mais um amanhecer em isolamento social. em Pra...</td>\n",
              "      <td>0</td>\n",
              "      <td>176</td>\n",
              "      <td>pt</td>\n",
              "      <td>Jaboat√£o dos Guararapes</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-34.9196</td>\n",
              "      <td>-8.19814</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74600</th>\n",
              "      <td>2020-06-02 16:04:51</td>\n",
              "      <td>Fot√≥grafo</td>\n",
              "      <td>omarcelogoulart</td>\n",
              "      <td>Em meio a uma pandemia, estamos na luta tamb√©m...</td>\n",
              "      <td>0</td>\n",
              "      <td>726</td>\n",
              "      <td>pt</td>\n",
              "      <td>S√£o F√©lix do Araguaia</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-53</td>\n",
              "      <td>-11</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8708 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    data/0  ...                                            data/12\n",
              "0                     data  ...                                           hashtags\n",
              "1      2020-03-19 17:49:40  ...                                                 []\n",
              "11     2020-04-16 00:34:54  ...                                                 []\n",
              "12     2020-05-03 13:48:52  ...  ['emcasa', 'isolamentosocial', 'saude', 'quero...\n",
              "13     2020-03-31 17:52:37  ...                                                 []\n",
              "...                    ...  ...                                                ...\n",
              "74569  2020-04-16 19:45:29  ...                                                 []\n",
              "74580  2020-05-31 12:38:32  ...                                                 []\n",
              "74581  2020-03-23 23:48:19  ...                                         ['Repost']\n",
              "74596  2020-05-12 09:30:34  ...                                                 []\n",
              "74600  2020-06-02 16:04:51  ...                                                 []\n",
              "\n",
              "[8708 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnvAMR5g8-8s"
      },
      "source": [
        "Dos dados com local, foi poss√≠vel notar a presen√ßa de diversos pa√≠ses interagindo com a not√≠cia que foi o corona no p√©riodo da coleta de dados, para melhorar a filtragem para o projeto foram separados apenas os dados que possuiam o idioma portugu√™s, ademais formata√ß√µes extras foram adicionadas para auxiliar na interpreta√ß√£o do projeto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraJCj2payBr"
      },
      "source": [
        "#### Organiza√ß√£o por Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl1IAdND-QrQ",
        "outputId": "82b49b18-70be-4dfc-bc7d-ab515e6e1358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvComLocal)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "limparArrays()\n",
        "\n",
        "for i in range(num_dataset_filtrado):\n",
        "  # Os dados foram separados em arrays para facilitar a ordena√ß√£o\n",
        "  dicTexto.append(dados.csvComLocal.iloc[i][3].lower())\n",
        "  dicLugar.append(dados.csvComLocal.iloc[i][7].lower())\n",
        "  dicPais.append(dados.csvComLocal.iloc[i][8].lower())\n",
        "  dicLatitude.append(dados.csvComLocal.iloc[i][10])\n",
        "  dicLongitude.append(dados.csvComLocal.iloc[i][11])\n",
        "  dicApelido.append(dados.csvComLocal.iloc[i][2].lower())\n",
        "  dicSigla.append(dados.csvComLocal.iloc[i][9])\n",
        "  dicIdioma.append(dados.csvComLocal.iloc[i][6].lower())\n",
        "  # um marcador foi adicionado a data\n",
        "  dicData.append(dados.csvComLocal.iloc[i][0] + \"+\" + str(i))\n",
        "\n",
        "# um sort foi feito com base no hor√°rio para a reordena√ß√£o\n",
        "dicData.sort()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "limparArrays() Arrays Limpos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3D3_sEX_t7V"
      },
      "source": [
        "# Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "for i in range(len(dicData)):\n",
        "  posicao = dicData[i].find(\"+\")\n",
        "  posicao_maxima = len(dicData[i])\n",
        "  horario_formatado = dicData[i][0:posicao]\n",
        "  posicao_formatado = int(dicData[i][posicao+1:posicao_maxima])\n",
        "  try:\n",
        "    # a filtragem dos dados que est√£o em lingua portuguesa foi feita nesse\n",
        "    # momento devido a um erro de indexError\n",
        "    if (dicIdioma!='pt'):\n",
        "      data_list = {'id': str(i),'data':horario_formatado,'texto':dicTexto[posicao_formatado],'apelido':dicApelido[posicao_formatado],'idioma':dicIdioma[posicao_formatado],'pais':dicPais[posicao_formatado],'latitude':dicLatitude[posicao_formatado],'longitude':dicLongitude[posicao_formatado],'sigla':dicSigla[posicao_formatado],'lugar':dicLugar[posicao_formatado]}\n",
        "      obj_para_download.append(data_list)\n",
        "  except IndexError:\n",
        "   print(\"index error em \",posicao_formatado,posicao)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3CVpv9ZAoiz",
        "outputId": "9c119d69-6fb3-4a7b-d5f5-bf5871b6f2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(obj_para_download)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>data</th>\n",
              "      <th>texto</th>\n",
              "      <th>apelido</th>\n",
              "      <th>idioma</th>\n",
              "      <th>pais</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>sigla</th>\n",
              "      <th>lugar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2020-01-07 17:10:48</td>\n",
              "      <td>#postagem3\\r\\n#janeirobranco \\r\\n*d√™ aten√ß√£o a...</td>\n",
              "      <td>santiagosjacome</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-37.2751</td>\n",
              "      <td>-7.0192</td>\n",
              "      <td>BR</td>\n",
              "      <td>patos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-08 03:13:50</td>\n",
              "      <td>larp \"quarentena 64\", sesc taubat√©, 2019. #lar...</td>\n",
              "      <td>confrariaideias</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-45.5679855</td>\n",
              "      <td>-23.00484244</td>\n",
              "      <td>BR</td>\n",
              "      <td>taubat√©</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2020-01-09 15:49:33</td>\n",
              "      <td>investiga√ß√£o identifica doen√ßa respirat√≥ria ch...</td>\n",
              "      <td>tsfradio</td>\n",
              "      <td>pt</td>\n",
              "      <td>portugal</td>\n",
              "      <td>-9.09904258</td>\n",
              "      <td>38.74751841</td>\n",
              "      <td>PT</td>\n",
              "      <td>lisbon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2020-01-09 23:12:23</td>\n",
              "      <td>#tipoftheday in #pneumologia  #üì∞ sobre o surto...</td>\n",
              "      <td>pulmaosa</td>\n",
              "      <td>pt</td>\n",
              "      <td>people's republic of china</td>\n",
              "      <td>114.35014953</td>\n",
              "      <td>30.52502419</td>\n",
              "      <td>CN</td>\n",
              "      <td>hubei</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2020-01-14 01:01:54</td>\n",
              "      <td>o isolamento social √© um problema cada vez mai...</td>\n",
              "      <td>fcpdiass</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-38.52180864</td>\n",
              "      <td>-13.00549888</td>\n",
              "      <td>BR</td>\n",
              "      <td>salvador</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8703</th>\n",
              "      <td>8703</td>\n",
              "      <td>2020-06-30 23:44:26</td>\n",
              "      <td>deus sabe de todas as coisas... #boanoite #deu...</td>\n",
              "      <td>luciana075</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-56.0969</td>\n",
              "      <td>-15.5958</td>\n",
              "      <td>BR</td>\n",
              "      <td>cuiab√°</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8704</th>\n",
              "      <td>8704</td>\n",
              "      <td>2020-06-30 23:53:01</td>\n",
              "      <td>eu treino,porque algo que me deixa completamen...</td>\n",
              "      <td>ligiagismara</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-38.602466</td>\n",
              "      <td>-4.490472</td>\n",
              "      <td>BR</td>\n",
              "      <td>ocara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8705</th>\n",
              "      <td>8705</td>\n",
              "      <td>2020-06-30 23:53:04</td>\n",
              "      <td>#fa√ßasuaparte\\r\\n#coronavirus \\r\\n#covid19 em ...</td>\n",
              "      <td>ricardofalchett</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-49.3794</td>\n",
              "      <td>-20.8102</td>\n",
              "      <td>BR</td>\n",
              "      <td>s√£o jos√© do rio preto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8706</th>\n",
              "      <td>8706</td>\n",
              "      <td>2020-06-30 23:57:22</td>\n",
              "      <td>boletim epidemiol√≥gico!!!\\r\\n\\r\\nribas do rio ...</td>\n",
              "      <td>eusigo90fm</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-53.7648</td>\n",
              "      <td>-20.4523</td>\n",
              "      <td>BR</td>\n",
              "      <td>ribas do rio pardo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8707</th>\n",
              "      <td>8707</td>\n",
              "      <td>data</td>\n",
              "      <td>texto</td>\n",
              "      <td>apelido</td>\n",
              "      <td>idioma</td>\n",
              "      <td>pais</td>\n",
              "      <td>latitude</td>\n",
              "      <td>longitude</td>\n",
              "      <td>sigla</td>\n",
              "      <td>lugar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8708 rows √ó 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                 data  ...  sigla                  lugar\n",
              "0        0  2020-01-07 17:10:48  ...     BR                  patos\n",
              "1        1  2020-01-08 03:13:50  ...     BR                taubat√©\n",
              "2        2  2020-01-09 15:49:33  ...     PT                 lisbon\n",
              "3        3  2020-01-09 23:12:23  ...     CN                  hubei\n",
              "4        4  2020-01-14 01:01:54  ...     BR               salvador\n",
              "...    ...                  ...  ...    ...                    ...\n",
              "8703  8703  2020-06-30 23:44:26  ...     BR                 cuiab√°\n",
              "8704  8704  2020-06-30 23:53:01  ...     BR                  ocara\n",
              "8705  8705  2020-06-30 23:53:04  ...     BR  s√£o jos√© do rio preto\n",
              "8706  8706  2020-06-30 23:57:22  ...     BR     ribas do rio pardo\n",
              "8707  8707                 data  ...  sigla                  lugar\n",
              "\n",
              "[8708 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN04Ygi6GRPk"
      },
      "source": [
        "#@title Baixar DadosEmPtComLocal.csv  { form-width: \"45%\" }\n",
        "\n",
        "gerarDownload = False #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosEmPtComLocal.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla','lugar'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zzdK3bZIuJi"
      },
      "source": [
        "# Os dados gerados para serem acessados:\n",
        "dados.csvEmPt = pd.read_csv(\"https://github.com/Emiliandro/AtvLCD_M2020_1/blob/master/separando-csv/exports-csv/DadosEmPtComLocal.csv?raw=true\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELy56FHKI8Eg"
      },
      "source": [
        "## Remo√ß√£o de Duplicatas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RWXg4WtnBdA"
      },
      "source": [
        "A primeira ideia para a remo√ß√£o foi seguindo as regras do twitter de que um usu√°rio n√£o pode postar o mesmo tweet do mesmo segundo, assim, removendo duplicatas na inser√ß√£o, contudo, durante a processo de observa√ß√£o foi percept√≠vel casos como o de site de not√≠cias: mesmo tweet em hor√°rios diferentes. Por conta disso, foi feito o processo de organiza√ß√£o por apelido.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBzDGmxDn-bn",
        "outputId": "d137f41c-a5dc-4424-ca5e-08d019da875c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# visualiza√ß√£o dos dados atuais\n",
        "pd.DataFrame(dados.csvEmPt).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>posicao</th>\n",
              "      <th>apelido</th>\n",
              "      <th>data</th>\n",
              "      <th>texto</th>\n",
              "      <th>idioma</th>\n",
              "      <th>pais</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>sigla</th>\n",
              "      <th>lugar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>santiagosjacome</td>\n",
              "      <td>2020-01-07 17:10:48</td>\n",
              "      <td>#postagem3\\r\\n#janeirobranco \\r\\n*d√™ aten√ß√£o a...</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-37.2751</td>\n",
              "      <td>-7.0192</td>\n",
              "      <td>BR</td>\n",
              "      <td>patos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>confrariaideias</td>\n",
              "      <td>2020-01-08 03:13:50</td>\n",
              "      <td>larp \"quarentena 64\", sesc taubat√©, 2019. #lar...</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-45.5679855</td>\n",
              "      <td>-23.00484244</td>\n",
              "      <td>BR</td>\n",
              "      <td>taubat√©</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>tsfradio</td>\n",
              "      <td>2020-01-09 15:49:33</td>\n",
              "      <td>investiga√ß√£o identifica doen√ßa respirat√≥ria ch...</td>\n",
              "      <td>pt</td>\n",
              "      <td>portugal</td>\n",
              "      <td>-9.09904258</td>\n",
              "      <td>38.74751841</td>\n",
              "      <td>PT</td>\n",
              "      <td>lisbon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>pulmaosa</td>\n",
              "      <td>2020-01-09 23:12:23</td>\n",
              "      <td>#tipoftheday in #pneumologia  #üì∞ sobre o surto...</td>\n",
              "      <td>pt</td>\n",
              "      <td>people's republic of china</td>\n",
              "      <td>114.35014953</td>\n",
              "      <td>30.52502419</td>\n",
              "      <td>CN</td>\n",
              "      <td>hubei</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>fcpdiass</td>\n",
              "      <td>2020-01-14 01:01:54</td>\n",
              "      <td>o isolamento social √© um problema cada vez mai...</td>\n",
              "      <td>pt</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-38.52180864</td>\n",
              "      <td>-13.00549888</td>\n",
              "      <td>BR</td>\n",
              "      <td>salvador</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   posicao          apelido                 data  ...     longitude sigla     lugar\n",
              "0        0  santiagosjacome  2020-01-07 17:10:48  ...       -7.0192    BR     patos\n",
              "1        1  confrariaideias  2020-01-08 03:13:50  ...  -23.00484244    BR   taubat√©\n",
              "2        2         tsfradio  2020-01-09 15:49:33  ...   38.74751841    PT    lisbon\n",
              "3        3         pulmaosa  2020-01-09 23:12:23  ...   30.52502419    CN     hubei\n",
              "4        4         fcpdiass  2020-01-14 01:01:54  ...  -13.00549888    BR  salvador\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x-UXVM6bIKB"
      },
      "source": [
        "#### Organiza√ß√£o por ordem alfab√©tica de apelido"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF8S9WUSnXhV",
        "outputId": "12753416-aead-4463-ea13-8ed07a377408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvEmPt)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "limparArrays()\n",
        "\n",
        "for i in range(num_dataset_filtrado):\n",
        "  # Os dados foram separados em arrays para facilitar a ordena√ß√£o\n",
        "  dicTexto.append(dados.csvEmPt.iloc[i]['texto'].lower())\n",
        "  dicLugar.append(dados.csvEmPt.iloc[i]['lugar'].lower())\n",
        "  dicPais.append(dados.csvEmPt.iloc[i]['pais'].lower())\n",
        "  dicLatitude.append(dados.csvEmPt.iloc[i]['latitude'])\n",
        "  dicLongitude.append(dados.csvEmPt.iloc[i]['longitude'])\n",
        "  dicSigla.append(dados.csvEmPt.iloc[i]['sigla'])\n",
        "  dicIdioma.append(dados.csvEmPt.iloc[i]['idioma'].lower())\n",
        "  dicData.append(dados.csvEmPt.iloc[i]['data'])\n",
        "  # um marcador foi adicionado a data\n",
        "  dicApelido.append(dados.csvEmPt.iloc[i]['apelido'].lower() + \"+\" + str(i))\n",
        "\n",
        "# um sort foi feito com base no hor√°rio para a reordena√ß√£o\n",
        "dicApelido.sort()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "limparArrays() Arrays Limpos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-D48H2Kns3A",
        "outputId": "c44f6474-83e8-4424-c377-7f0b9b994904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "for i in range(len(dicApelido)):\n",
        "  posicao = dicApelido[i].find(\"+\")\n",
        "  posicao_maxima = len(dicApelido[i])\n",
        "  apelido_formatado = dicApelido[i][0:posicao]\n",
        "  posicao_formatado = int(dicApelido[i][posicao+1:posicao_maxima])\n",
        "  try:\n",
        "    # a filtragem dos dados que est√£o em lingua portuguesa foi feita nesse\n",
        "    # momento devido a um erro de indexError\n",
        "    if (dicIdioma!='pt'):\n",
        "      data_list = {'id': str(i),'apelido':apelido_formatado,\n",
        "                   'texto':dicTexto[posicao_formatado],\n",
        "                   'data':dicData[posicao_formatado],\n",
        "                   'idioma':dicIdioma[posicao_formatado],\n",
        "                   'pais':dicPais[posicao_formatado],\n",
        "                   'latitude':dicLatitude[posicao_formatado],\n",
        "                   'longitude':dicLongitude[posicao_formatado],\n",
        "                   'sigla':dicSigla[posicao_formatado],\n",
        "                   'lugar':dicLugar[posicao_formatado]}\n",
        "      obj_para_download.append(data_list)\n",
        "  except IndexError:\n",
        "   print(\"index error em \",posicao_formatado,posicao)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8a2db0126c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mposicao_maxima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicApelido\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mapelido_formatado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicApelido\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mposicao\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mposicao_formatado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicApelido\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposicao\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mposicao_maxima\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# a filtragem dos dados que est√£o em lingua portuguesa foi feita nesse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '02e9b7e20f9844a'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgXXS2tpDZF"
      },
      "source": [
        "# visualiza√ß√£o dos dados ordenados\n",
        "\n",
        "pd.DataFrame(obj_para_download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D6Rcv9CpSE9"
      },
      "source": [
        "#@title Baixar dados n√£o nulos ordenado por apelido em .csv  { form-width: \"45%\" }\n",
        "\n",
        "\n",
        "gerarDownload = True #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosNaoNuloFrmtApelido.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla','lugar'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIfL1fqeq_Nb"
      },
      "source": [
        "# Os dados gerados para serem acessados:\n",
        "dados.csvOrdemNome = pd.read_csv(\"https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/separando-csv/exports-csv/DadosNaoNuloFrmtApelido.csv\")\n",
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(dados.csvOrdemNome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFtAQHybt_qv"
      },
      "source": [
        "### Primeira Filtragem: remo√ß√£o de textos repetidos de mesmo usu√°rio \n",
        "Tendo ordenado por nome, √© filtrado os valores de texto repetidos com em seus textos, por meio de tentativa e erro foi desenvolvido uma l√≥gica em torno de comparar os primerios caract√©res do texto p√∫blico sem espa√ßamentos, assim, foi feita a remo√ß√£o de repostagens de sites de not√≠cias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyOzozf4s8D0"
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvOrdemNome)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "\n",
        "limparArrays()\n",
        "\n",
        "filtro1 = \"\"\n",
        "filtro2 = \"\"\n",
        "valor = 0\n",
        "dicLugar.append(dados.csvOrdemNome.iloc[valor]['lugar'].lower())\n",
        "dicPais.append(dados.csvOrdemNome.iloc[valor]['pais'].lower())\n",
        "dicLatitude.append(dados.csvOrdemNome.iloc[valor]['latitude'])\n",
        "dicLongitude.append(dados.csvOrdemNome.iloc[valor]['longitude'])\n",
        "dicSigla.append(dados.csvOrdemNome.iloc[valor]['sigla'])\n",
        "dicIdioma.append(dados.csvOrdemNome.iloc[valor]['idioma'].lower())\n",
        "dicData.append(dados.csvOrdemNome.iloc[valor]['data'])\n",
        "dicApelido.append(dados.csvOrdemNome.iloc[valor]['apelido'].lower())\n",
        "dicTexto.append(dados.csvOrdemNome.iloc[valor]['texto'].lower())\n",
        "\n",
        "texto_0 = dados.csvOrdemNome.iloc[valor]['texto'].lower()\n",
        "edit_texto_0 = texto_0.translate({ord(c): None for c in string.whitespace})\n",
        "\n",
        "dicResumo.append(edit_texto_0[0:25])\n",
        "\n",
        "filtro1 = dados.csvOrdemNome.iloc[0]['texto'].lower()\n",
        "repetidos = 0\n",
        "\n",
        "for valor in range(num_dataset_filtrado):\n",
        "  if (valor>0):\n",
        "    #se n√£o for a primeira execu√ß√£o\n",
        "    filtro2 = dados.csvOrdemNome.iloc[valor]['texto'].lower()\n",
        "    edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})\n",
        "    edit_filtro2 = filtro2.translate({ord(c): None for c in string.whitespace})\n",
        "    print(dados.csvOrdemNome.iloc[valor]['apelido'].lower(),edit_filtro1[0:20]==edit_filtro2[0:20],edit_filtro2)\n",
        "    if (edit_filtro1[0:25]==edit_filtro2[0:25]):\n",
        "      #valores s√£o iguais\n",
        "      repetidos += 1\n",
        "    else:\n",
        "      #adicionar novo valor, pois\n",
        "      # valores s√£o diferentes\n",
        "      dicResumo.append(edit_filtro2[0:25])\n",
        "      dicLugar.append(dados.csvOrdemNome.iloc[valor]['lugar'].lower())\n",
        "      dicPais.append(dados.csvOrdemNome.iloc[valor]['pais'].lower())\n",
        "      dicLatitude.append(dados.csvOrdemNome.iloc[valor]['latitude'])\n",
        "      dicLongitude.append(dados.csvOrdemNome.iloc[valor]['longitude'])\n",
        "      dicSigla.append(dados.csvOrdemNome.iloc[valor]['sigla'])\n",
        "      dicIdioma.append(dados.csvOrdemNome.iloc[valor]['idioma'].lower())\n",
        "      dicData.append(dados.csvOrdemNome.iloc[valor]['data'])\n",
        "      dicApelido.append(dados.csvOrdemNome.iloc[valor]['apelido'].lower())\n",
        "      dicTexto.append(dados.csvOrdemNome.iloc[valor]['texto'].lower())\n",
        "      #atualizar valor filtro1\n",
        "      filtro1 = dados.csvOrdemNome.iloc[valor]['texto'].lower()\n",
        "print(str(len(dicTexto)))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytVZeMruFjd"
      },
      "source": [
        " # Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "for i in range(len(dicTexto)):\n",
        "    posicao_formatado = i\n",
        "    data_list = {'id': str(i),'apelido':dicApelido[i],\n",
        "                  'texto':dicTexto[posicao_formatado],\n",
        "                  'data':dicData[posicao_formatado],'idioma':dicIdioma[posicao_formatado],\n",
        "                  'pais':dicPais[posicao_formatado],'latitude':dicLatitude[posicao_formatado],\n",
        "                  'longitude':dicLongitude[posicao_formatado],'sigla':dicSigla[posicao_formatado],\n",
        "                  'lugar':dicLugar[posicao_formatado],'resumo':dicResumo[posicao_formatado]}\n",
        "    obj_para_download.append(data_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKqkSGzUuW_f"
      },
      "source": [
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(obj_para_download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6LHAkUJmqE1"
      },
      "source": [
        "#@title Baixar dados duplicados com resumo e sem texto repetido por usu√°rio em .csv  { form-width: \"45%\" }\n",
        "\n",
        "gerarDownload = True #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosRemDuplFiltragemComResumo2.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla',\n",
        "                     'lugar','resumo'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'],value['resumo'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVslJP_9mKmU"
      },
      "source": [
        "### Segunda filtragem: remo√ß√£o de textos repetidos de diferentes usu√°rios\n",
        "A redu√ß√£o dos dados contudo levou a constatar que n√£o era o suficiente, pois usu√°rios diferentes poderiam ter postado a mesma mensagem, o que √© comum em caso de compartilhamentos em massa. Assim, uma nova filtragem se tornou necess√°ria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xblrAcZ3bhDO"
      },
      "source": [
        "##### Organiza√ß√£o por ordem alfab√©tica de textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAb3LjWnOxY"
      },
      "source": [
        "# Os dados gerados para serem acessados:\n",
        "dados.csvFiltragem2 = pd.read_csv(\"https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/separando-csv/exports-csv/DadosRemDuplFiltragemComResumo2.csv\")\n",
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(dados.csvFiltragem2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2bVWovoMo6j"
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvFiltragem2)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "\n",
        "limparArrays()\n",
        "\n",
        "for i in range(num_dataset_filtrado):\n",
        "  # Os dados foram separados em arrays para facilitar a ordena√ß√£o\n",
        "  dicTexto.append(dados.csvFiltragem2.iloc[i]['texto'].lower())\n",
        "  dicLugar.append(dados.csvFiltragem2.iloc[i]['lugar'].lower())\n",
        "  dicPais.append(dados.csvFiltragem2.iloc[i]['pais'].lower())\n",
        "  dicLatitude.append(dados.csvFiltragem2.iloc[i]['latitude'])\n",
        "  dicLongitude.append(dados.csvFiltragem2.iloc[i]['longitude'])\n",
        "  dicSigla.append(dados.csvFiltragem2.iloc[i]['sigla'])\n",
        "  dicIdioma.append(dados.csvFiltragem2.iloc[i]['idioma'].lower())\n",
        "  dicApelido.append(dados.csvFiltragem2.iloc[i]['apelido'].lower())\n",
        "  dicData.append(dados.csvFiltragem2.iloc[i]['data'])\n",
        "  # um marcador foi adicionado a data\n",
        "  dicResumo.append(dados.csvFiltragem2.iloc[i]['resumo'].lower() + \"+\" + str(i))\n",
        "\n",
        "# um sort foi feito com base no hor√°rio para a reordena√ß√£o\n",
        "dicResumo.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4tC_D0MNNhX"
      },
      "source": [
        "# Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "contador_de_erros = 0\n",
        "for i in range(len(dicResumo)):\n",
        "  posicao = dicResumo[i].find(\"+\")\n",
        "  posicao_maxima = len(dicResumo[i])\n",
        "  resumo_formatado = dicResumo[i][0:posicao]\n",
        "  try:\n",
        "    posicao_formatado = int(dicResumo[i][posicao+1:posicao_maxima])\n",
        "    try:\n",
        "      # a filtragem dos dados que est√£o em lingua portuguesa foi feita nesse\n",
        "      # momento devido a um erro de indexError\n",
        "      data_list = {'id': str(i),'apelido':dicApelido[posicao_formatado],\n",
        "                  'texto':dicTexto[posicao_formatado],\n",
        "                  'data':dicData[posicao_formatado],\n",
        "                  'idioma':dicIdioma[posicao_formatado],\n",
        "                  'pais':dicPais[posicao_formatado],\n",
        "                  'latitude':dicLatitude[posicao_formatado],\n",
        "                  'longitude':dicLongitude[posicao_formatado],\n",
        "                  'sigla':dicSigla[posicao_formatado],\n",
        "                  'lugar':dicLugar[posicao_formatado],\n",
        "                  'resumo':resumo_formatado}\n",
        "      obj_para_download.append(data_list)\n",
        "    except IndexError:\n",
        "      print(\"index error em \",posicao_formatado,posicao)\n",
        "  except (ValueError):\n",
        "    contador_de_erros +=1\n",
        "    print(str(contador_de_erros),dicTexto[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3fTUEH-PbAA"
      },
      "source": [
        "Obs: durante a filtragem houveram erros por conta do m√©todo de filtragem, essas foram deixadas ser cortadas pois nenhuma possuia informa√ß√£o relevante para o trabalho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKHfsMQUNxiR"
      },
      "source": [
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(obj_para_download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZTFyolINu0V"
      },
      "source": [
        "#@title Baixar dados duplicados com resumo ordenados por ordem alfabetica de texto e sem texto repetido .csv  { form-width: \"45%\" }\n",
        "\n",
        "gerarDownload = True #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosRemDuplFiltragemComResumo3.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla',\n",
        "                     'lugar','resumo'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'],value['resumo'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH6ywIlVPxcZ"
      },
      "source": [
        "# Os dados gerados para serem acessados:\n",
        "dados.csvFiltragem3 = pd.read_csv(\"https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/separando-csv/exports-csv/DadosRemDuplFiltragemComResumo3.csv\")\n",
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(dados.csvFiltragem3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnaROrAZb0Au"
      },
      "source": [
        "##### Remo√ß√£o de Textos Repetidos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSErwi60OhBc"
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvFiltragem3)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "limparArrays()\n",
        "\n",
        "filtro1 = \"\"\n",
        "filtro2 = \"\"\n",
        "valor = 0\n",
        "dicLugar.append(dados.csvFiltragem3.iloc[valor]['lugar'].lower())\n",
        "dicPais.append(dados.csvFiltragem3.iloc[valor]['pais'].lower())\n",
        "dicLatitude.append(dados.csvFiltragem3.iloc[valor]['latitude'])\n",
        "dicLongitude.append(dados.csvFiltragem3.iloc[valor]['longitude'])\n",
        "dicSigla.append(dados.csvFiltragem3.iloc[valor]['sigla'])\n",
        "dicIdioma.append(dados.csvFiltragem3.iloc[valor]['idioma'].lower())\n",
        "dicData.append(dados.csvFiltragem3.iloc[valor]['data'])\n",
        "dicApelido.append(dados.csvFiltragem3.iloc[valor]['apelido'].lower())\n",
        "dicTexto.append(dados.csvFiltragem3.iloc[valor]['texto'].lower())\n",
        "\n",
        "texto_0 = dados.csvFiltragem3.iloc[valor]['texto'].lower()\n",
        "edit_texto_0 = texto_0.translate({ord(c): None for c in string.whitespace})\n",
        "\n",
        "dicResumo.append(edit_texto_0[0:25])\n",
        "\n",
        "filtro1 = dados.csvFiltragem3.iloc[0]['texto'].lower()\n",
        "repetidos = 0\n",
        "\n",
        "for valor in range(num_dataset_filtrado):\n",
        "  if (valor>0):\n",
        "    #se n√£o for a primeira execu√ß√£o\n",
        "    filtro2 = dados.csvFiltragem3.iloc[valor]['texto'].lower()\n",
        "    edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})\n",
        "    edit_filtro2 = filtro2.translate({ord(c): None for c in string.whitespace})\n",
        "    print(dados.csvFiltragem3.iloc[valor]['apelido'].lower(),edit_filtro1[0:20]==edit_filtro2[0:20],edit_filtro2)\n",
        "    if (edit_filtro1[0:25]==edit_filtro2[0:25]):\n",
        "      #valores s√£o iguais\n",
        "      repetidos += 1\n",
        "    else:\n",
        "      #adicionar novo valor, pois\n",
        "      # valores s√£o diferentes\n",
        "      dicResumo.append(edit_filtro2[0:25])\n",
        "      dicLugar.append(dados.csvFiltragem3.iloc[valor]['lugar'].lower())\n",
        "      dicPais.append(dados.csvFiltragem3.iloc[valor]['pais'].lower())\n",
        "      dicLatitude.append(dados.csvFiltragem3.iloc[valor]['latitude'])\n",
        "      dicLongitude.append(dados.csvFiltragem3.iloc[valor]['longitude'])\n",
        "      dicSigla.append(dados.csvFiltragem3.iloc[valor]['sigla'])\n",
        "      dicIdioma.append(dados.csvFiltragem3.iloc[valor]['idioma'].lower())\n",
        "      dicData.append(dados.csvFiltragem3.iloc[valor]['data'])\n",
        "      dicApelido.append(dados.csvFiltragem3.iloc[valor]['apelido'].lower())\n",
        "      dicTexto.append(dados.csvFiltragem3.iloc[valor]['texto'].lower())\n",
        "      #atualizar valor filtro1\n",
        "      filtro1 = dados.csvFiltragem3.iloc[valor]['texto'].lower()\n",
        "print(str(len(dicTexto)))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cukGPbOrQ0L4"
      },
      "source": [
        " # Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "for i in range(len(dicTexto)):\n",
        "    posicao_formatado = i\n",
        "    data_list = {'id': str(i),'apelido':dicApelido[i],\n",
        "                  'texto':dicTexto[posicao_formatado],\n",
        "                  'data':dicData[posicao_formatado],'idioma':dicIdioma[posicao_formatado],\n",
        "                  'pais':dicPais[posicao_formatado],'latitude':dicLatitude[posicao_formatado],\n",
        "                  'longitude':dicLongitude[posicao_formatado],'sigla':dicSigla[posicao_formatado],\n",
        "                  'lugar':dicLugar[posicao_formatado],'resumo':dicResumo[posicao_formatado]}\n",
        "    obj_para_download.append(data_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2vRrfLtQ7O3"
      },
      "source": [
        "#@title Baixar vers√£o 2 da filtragem de duplicatas em .csv  { form-width: \"45%\" }\n",
        "\n",
        "gerarDownload = True #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosRemDuplFiltragemV2.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla',\n",
        "                     'lugar','resumo'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'],value['resumo'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_I1dduRf86"
      },
      "source": [
        "## Resultado da Remo√ß√£o de Duplicatas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftBB3VHzRnff"
      },
      "source": [
        "# Os dados gerados para serem acessados:\n",
        "dados.csvFiltragem4 = pd.read_csv(\"https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/separando-csv/exports-csv/DadosRemDuplFiltragemV2.csv\")\n",
        "# visualiza√ß√£o dos dados ordenados\n",
        "pd.DataFrame(dados.csvFiltragem4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9fqjiKT7Ii"
      },
      "source": [
        "\n",
        "## Somente Perguntas\n",
        "\n",
        "(...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPC15saqbZPj"
      },
      "source": [
        "num_dataset_filtrado = len(dados.csvFiltragem4)\n",
        "\n",
        "# Nessa parte foi filtrada os dados presentes do CSV com os dados n√£o nulos\n",
        "limparArrays()\n",
        "ids_com_interrogacao = []\n",
        "ids_com_quem = []\n",
        "ids_com_qual = []\n",
        "ids_com_pq = []\n",
        "ids_com_sabe = []\n",
        "\n",
        "num_com_interrogacao = 0\n",
        "num_com_quem = 0\n",
        "num_com_qual = 0\n",
        "num_com_pq = 0\n",
        "num_com_sabe = 0\n",
        "\n",
        "for i in range(num_dataset_filtrado):\n",
        "  if (i>=0):\n",
        "    #se n√£o for a primeira execu√ß√£o\n",
        "    # problema encontrado: muitas frases s√£o cita√ß√µes, logo irrelevante\n",
        "    filtro1 = dados.csvFiltragem4.iloc[i]['texto'].lower()\n",
        "    edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})\n",
        "    try:\n",
        "      valor = filtro1.find(\"?\")\n",
        "      if (valor!=-1):\n",
        "        num_com_interrogacao +=1\n",
        "        ids_com_interrogacao.append(i)\n",
        "    except:\n",
        "      print(erro)\n",
        "    try:\n",
        "      valor = filtro1.find(\"quem\")\n",
        "      if (valor!=-1):\n",
        "        num_com_quem +=1\n",
        "        ids_com_quem.append(i)\n",
        "    except:\n",
        "      print(erro)\n",
        "    try:\n",
        "      valor = filtro1.find(\"qual\")\n",
        "      if (valor!=-1):\n",
        "        num_com_qual +=1\n",
        "        ids_com_qual.append(i)\n",
        "    except:\n",
        "      print(erro)\n",
        "    try:\n",
        "      valor = filtro1.find(\"sabe\")\n",
        "      if (valor!=-1):\n",
        "        num_com_sabe +=1\n",
        "        ids_com_sabe.append(i)\n",
        "    except:\n",
        "      print(erro)\n",
        "    try:\n",
        "      valor = filtro1.find(\"pq\")\n",
        "      if (valor!=-1):\n",
        "        num_com_pq +=1\n",
        "        ids_com_pq.append(i)\n",
        "    except:\n",
        "      print(erro)\n",
        "\n",
        "print(\"num_com_interrogacao\",\"total\", num_com_interrogacao,ids_com_interrogacao)\n",
        "print(\"num_com_quem\",\"total\", num_com_quem,ids_com_quem)\n",
        "print(\"num_com_qual\",\"total\", num_com_qual,ids_com_qual)\n",
        "print(\"num_com_sabe\",\"total\", num_com_sabe,ids_com_sabe)\n",
        "print(\"num_com_pq\",\"total\", num_com_pq,ids_com_pq)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FvNFjYYfHeP"
      },
      "source": [
        "#@title Baixar Somente Com Interrogacao em .csv  { form-width: \"25%\" }\n",
        "\n",
        "# Prepara√ß√£o do arquivo para download\n",
        "# e jun√ß√£o dos dados na ordem de data\n",
        "obj_para_download = []\n",
        "for i in range(len(ids_com_interrogacao)):\n",
        "    posicao_formatado = ids_com_interrogacao[i]\n",
        "    data_list = {'id': str(i),\n",
        "                 'apelido':dados.csvFiltragem4.iloc[posicao_formatado]['apelido'],\n",
        "                 'texto':dados.csvFiltragem4.iloc[posicao_formatado]['texto'],\n",
        "                 'data':dados.csvFiltragem4.iloc[posicao_formatado]['data'],\n",
        "                 'idioma':dados.csvFiltragem4.iloc[posicao_formatado]['idioma'],\n",
        "                 'pais':dados.csvFiltragem4.iloc[posicao_formatado]['pais'],\n",
        "                 'latitude':dados.csvFiltragem4.iloc[posicao_formatado]['latitude'],\n",
        "                 'longitude':dados.csvFiltragem4.iloc[posicao_formatado]['longitude'],\n",
        "                 'sigla':dados.csvFiltragem4.iloc[posicao_formatado]['sigla'],\n",
        "                 'lugar':dados.csvFiltragem4.iloc[posicao_formatado]['lugar'],\n",
        "                 'resumo':dados.csvFiltragem4.iloc[posicao_formatado]['resumo'],}\n",
        "    obj_para_download.append(data_list)\n",
        "\n",
        "\n",
        "gerarDownload = True #@param {type:\"boolean\"}\n",
        "dic_para_download = {}\n",
        "\n",
        "if(gerarDownload):\n",
        "  for di in obj_para_download:\n",
        "    dic_para_download[di['id']]={}\n",
        "    for k in di.keys():\n",
        "      if k =='id': continue\n",
        "      dic_para_download[di['id']][k]=di[k]\n",
        "      \n",
        "  with open('DadosSomenteComInterrogacao.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                     'idioma','pais','latitude','longitude','sigla',\n",
        "                     'lugar','resumo'])\n",
        "    for key, value in dic_para_download.items():\n",
        "      writer.writerow([key, value['apelido'], value['data'], \n",
        "                       value['texto'], value['idioma'],\n",
        "                       value['pais'], value['latitude'],\n",
        "                       value['longitude'], value['sigla'],\n",
        "                       value['lugar'],value['resumo'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoopPXGcT0gP"
      },
      "source": [
        "# Interpreta√ß√£o dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSg-A2EqWtX"
      },
      "source": [
        "### Script de Filtragem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-UniLmm2_M"
      },
      "source": [
        "\n",
        "def filtrar_e_exportar(csv_usado,formatar,palavra,exportar,nome):      \n",
        "  obj_para_download = []\n",
        "  dic_para_download = {}\n",
        "\n",
        "  # ----------------------\n",
        "  # FILTRANDO DADOS DOWNLOAD\n",
        "  # ----------------------\n",
        "  num_dataset_filtrado = len(dados.csvFiltragem4)\n",
        "  ids_filtrados = []\n",
        "  num_filtrados = 0\n",
        "  for i in range(num_dataset_filtrado):\n",
        "    filtro1 = dados.csvFiltragem4.iloc[i]['texto'].lower()\n",
        "    if (formatar):\n",
        "      edit_filtro1 = filtro1.translate({ord(c): None for c in string.whitespace})\n",
        "      filtro1 = edit_filtro1\n",
        "    if (i>=0):\n",
        "      try:\n",
        "        valor = filtro1.find(palavra)\n",
        "        if (valor!=-1):\n",
        "          num_filtrados +=1\n",
        "          ids_filtrados.append(i)\n",
        "      except:\n",
        "        print(\"erro\")\n",
        "    if(i==num_dataset_filtrado-1):\n",
        "      print(\"\\n Filtragem da palavra\",palavra,\"terminou\",\n",
        "            \"\\n Foram filtrados um total de\", num_filtrados, \"itens\",\n",
        "            \"\\n sendo eles\",ids_filtrados)\n",
        "      \n",
        "  # ----------------------\n",
        "  # FORMATAR DOWNLOAD\n",
        "  # ----------------------\n",
        "\n",
        "  for i in range(len(ids_com_interrogacao)):\n",
        "      posicao_formatado = ids_com_interrogacao[i]\n",
        "      data_list = {'id': str(i),\n",
        "                  'apelido':dados.csvFiltragem4.iloc[posicao_formatado]['apelido'],\n",
        "                  'texto':dados.csvFiltragem4.iloc[posicao_formatado]['texto'],\n",
        "                  'data':dados.csvFiltragem4.iloc[posicao_formatado]['data'],\n",
        "                  'idioma':dados.csvFiltragem4.iloc[posicao_formatado]['idioma'],\n",
        "                  'pais':dados.csvFiltragem4.iloc[posicao_formatado]['pais'],\n",
        "                  'latitude':dados.csvFiltragem4.iloc[posicao_formatado]['latitude'],\n",
        "                  'longitude':dados.csvFiltragem4.iloc[posicao_formatado]['longitude'],\n",
        "                  'sigla':dados.csvFiltragem4.iloc[posicao_formatado]['sigla'],\n",
        "                  'lugar':dados.csvFiltragem4.iloc[posicao_formatado]['lugar'],\n",
        "                  'resumo':dados.csvFiltragem4.iloc[posicao_formatado]['resumo'],}\n",
        "      obj_para_download.append(data_list)\n",
        "\n",
        "  # ----------------------\n",
        "  # EXPORTANDO CSV\n",
        "  # ----------------------\n",
        "\n",
        "  gerarDownload = exportar\n",
        "  nome_csv = \"datasheet_\" + str(palavra)\n",
        "  if(nome != \"\"):\n",
        "    nome_csv = nome_do_arquivo\n",
        "\n",
        "  if(gerarDownload):\n",
        "    for di in obj_para_download:\n",
        "      dic_para_download[di['id']]={}\n",
        "      for k in di.keys():\n",
        "        if k =='id': continue\n",
        "        dic_para_download[di['id']][k]=di[k]\n",
        "        \n",
        "    with open( nome_csv +'.csv', 'w') as csv_file:  \n",
        "      writer = csv.writer(csv_file)\n",
        "      writer.writerow(['posicao', 'apelido', 'data', 'texto', \n",
        "                      'idioma','pais','latitude','longitude','sigla',\n",
        "                      'lugar','resumo'])\n",
        "      for key, value in dic_para_download.items():\n",
        "        writer.writerow([key, value['apelido'], value['data'], \n",
        "                        value['texto'], value['idioma'],\n",
        "                        value['pais'], value['latitude'],\n",
        "                        value['longitude'], value['sigla'],\n",
        "                        value['lugar'],value['resumo'] ])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIhXeEgIqbDP"
      },
      "source": [
        "### Pesquisa no Filtro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbQLNlCtqNTg",
        "outputId": "0dad3076-9633-4da4-e07a-7736d9c11d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#@title Op√ß√µes de Filtragem { form-width: \"70%\" }\n",
        "qual_palavra = \"presidente\" #@param {type:\"string\"}\n",
        "formatar_texto = False #@param {type:\"boolean\"}\n",
        "exportar_csv = False #@param {type:\"boolean\"}\n",
        "nome_do_arquivo = \"pesquisa_oportunidades\" #@param {type:\"string\"}\n",
        "usar_csv_url = False #@param {type:\"boolean\"}\n",
        "url_do_csv = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if (usar_csv_url):\n",
        "  csv_para_usar = pd.read_csv(url_do_csv)\n",
        "else:\n",
        "  csv_para_usar: pd.read_csv(\"https://raw.githubusercontent.com/Emiliandro/AtvLCD_M2020_1/master/separando-csv/exports-csv/DadosRemDuplFiltragemV2.csv\")\n",
        "\n",
        "filtrar_e_exportar(csv_para_usar,False,qual_palavra,exportar_csv,nome_do_arquivo)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Filtragem da palavra presidente terminou \n",
            " Foram filtrados um total de 55 itens \n",
            " sendo eles [44, 569, 831, 1128, 1145, 1308, 1510, 2001, 3163, 3184, 3325, 3344, 3526, 3652, 3653, 3658, 3671, 3882, 3944, 3996, 4022, 4252, 4281, 4524, 4731, 4833, 4841, 4970, 5190, 5300, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5560, 5644, 5645, 5657, 6003, 6033, 6034, 6109, 6161, 6318, 6544, 6705, 6741, 7170, 7431, 7440, 7534]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}